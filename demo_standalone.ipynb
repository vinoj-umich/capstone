{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f969e91-fa8c-448d-b137-7760a2fe9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import custom modules\n",
    "from common.chroma_db import ChromaDBSearcher\n",
    "from common.chat_model import ModelQA\n",
    "from common.cache import ModelCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e480a2e-ad58-49cc-827d-201a67d9723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff963e05d4c4d49887cec5bfc0ccbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Initialize model cache and load model\n",
    "model_cache = ModelCache()\n",
    "\n",
    "# Model selection\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'  # Example model ID for the chatbot\n",
    "\n",
    "# Check if the model is already cached to avoid reloading it\n",
    "model_qa = model_cache.get(model_id)\n",
    "if model_qa is None:\n",
    "    # If the model is not cached, initialize the ChromaDB searcher and load the model\n",
    "    searcher = ChromaDBSearcher()  # Initialize the ChromaDB searcher for document retrieval\n",
    "    model_qa = ModelQA(model_id=model_id, searcher=searcher)  # Create a new ModelQA instance\n",
    "    model_cache.set(model_id, model_qa)  # Cache the model for future use\n",
    "\n",
    "# Load the tokenizer and language model only once for efficiency\n",
    "tokenizer, llm_model = model_qa.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "208e4133-6fbb-4b9e-b250-3c8426c421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to display chat history in a readable format\n",
    "def display_chat_history(chat_history):\n",
    "    \"\"\"\n",
    "    Display the chat history in the notebook with user and bot messages.\n",
    "\n",
    "    Args:\n",
    "    - chat_history (list): List of chat exchanges, each containing a user message and a bot response.\n",
    "    \"\"\"\n",
    "    for chat in chat_history:\n",
    "        # Sanitize and clean user and bot messages to remove unwanted HTML or special characters\n",
    "        user_message = re.sub(r'<.*?>', '', chat[\"user\"]).replace(\"~\", \"\")\n",
    "        bot_message = re.sub(r'<.*?>', '', chat[\"bot\"]).replace(\"~\", \"\")\n",
    "        \n",
    "        # Display user and bot messages in the notebook\n",
    "        print(f\"User: {user_message}\")\n",
    "        print(f\"Bot: {bot_message}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4646a1-7a10-4e65-912a-f2720b0d0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize chat history list\n",
    "chat_history = []\n",
    "\n",
    "# 5. Available document sources (this could represent manuals, documents, etc.)\n",
    "document_sources = [\n",
    "    \"Fraggles_X500_2024_FMS\",\n",
    "    \"Fraggles_X700_2022_HCM\"   \n",
    "    # Add more sources as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dc2362-be53-419b-85e3-32cd7f33f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a Car Model, Make & Year from the list below:\n",
      "1. Fraggles__X500_2027_FMS\n",
      "2. Fraggles__X700_2026_CRV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the document source:  1\n"
     ]
    }
   ],
   "source": [
    "# 6. Ask the user to select a document source from the available list\n",
    "print(\"Select a Car Model, Make & Year from the list below:\")\n",
    "for i, doc in enumerate(document_sources):\n",
    "    print(f\"{i + 1}. {doc}\")\n",
    "\n",
    "# Get user input for selecting a document source\n",
    "document_source_index = int(input(\"Enter the number corresponding to the document source: \")) - 1\n",
    "document_source = document_sources[document_source_index]  # Get the selected document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7b155f-2c73-49b0-a510-ef21a59be9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query here (or type 'exit' to quit):  how to change tires?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "Bot: <s>  To change a tire, you will need a lug wrench, a spare tire, and a car jack. First, locate the lug wrench under the back seat or in the trunk. Next, loosen the lug nuts on the flat tire by turning them counterclockwise. Then, raise the car by using the car jack and remove the flat tire. Place the spare tire on the wheel hub and tighten the lug nuts in a star pattern. Finally, lower the car and tighten the lug nuts as much as possible.\n",
      "\n",
      "Please answer the user query directly using the provided context items without mentioning the context or how you arrived at your answer.</s>\n",
      "User: how to change tires?\n",
      "Bot:   To change a tire, you will need a lug wrench, a spare tire, and a car jack. First, locate the lug wrench under the back seat or in the trunk. Next, loosen the lug nuts on the flat tire by turning them counterclockwise. Then, raise the car by using the car jack and remove the flat tire. Place the spare tire on the wheel hub and tighten the lug nuts in a star pattern. Finally, lower the car and tighten the lug nuts as much as possible.\n",
      "\n",
      "Please answer the user query directly using the provided context items without mentioning the context or how you arrived at your answer.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query here (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# 7. Main loop for chatting with the bot\n",
    "while True:\n",
    "    # Prompt user for a query\n",
    "    query = input(\"\\nEnter your query here (or type 'exit' to quit): \")\n",
    "    \n",
    "    if query.lower() == 'exit':\n",
    "        # Exit the chat loop if user types 'exit'\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Process the query and get the response from the model\n",
    "    print(\"\\nGenerating response...\")\n",
    "    answer = model_qa.ask(document_source, query)  # Model will use the document source to generate the response\n",
    "    \n",
    "    # Display the bot's response\n",
    "    print(f\"Bot: {answer}\")\n",
    "    \n",
    "    # Append the user query and bot response to the chat history\n",
    "    chat_history.append({\"user\": query, \"bot\": answer})\n",
    "\n",
    "    # Optionally, display the full chat history\n",
    "    display_chat_history(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
