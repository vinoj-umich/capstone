{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vinoj/.local/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vinoj/.local/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vinoj/.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vinoj/.local/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vinoj/.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vinoj/.local/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     import fitz\n",
    "# except ImportError:\n",
    "#     !pip install PyMuPDF\n",
    "#     !pip install fitz\n",
    "\n",
    "# try:\n",
    "#     import spacy\n",
    "# except ImportError:\n",
    "#     !pip install spacy\n",
    "\n",
    "# try:\n",
    "#     import sentencepiece\n",
    "# except ImportError:\n",
    "#     !pip install sentencepiece\n",
    "\n",
    "# try:\n",
    "#     import transformers\n",
    "# except:\n",
    "#     !pip install transformers\n",
    "\n",
    "#!pip install --upgrade numpy\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from tqdm.auto import tqdm  # For progress bars\n",
    "import re  # For regex operations\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer  # For embeddings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import logging\n",
    "import spacy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "def log_output(string):\n",
    "    #logger.info(string)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Text Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFormatter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        formatted_texts = []\n",
    "        \n",
    "        for pages_and_text in X:\n",
    "            # Replace newlines with spaces and strip leading/trailing spaces\n",
    "            formatted_text = pages_and_text['text'].replace(\"\\n\", \" \").strip()\n",
    "            formatted_page_text = {\"page_number\": pages_and_text['page_number'], \"formatted_text\": formatted_text}\n",
    "            formatted_texts.append(formatted_page_text)\n",
    "\n",
    "        log_output(\"Formatted texts successfully.\")\n",
    "        return formatted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Open and Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFReader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        \"\"\"\n",
    "        Reads a PDF file and extracts text from each page.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing the page number and its text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(self.pdf_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to open PDF file: {self.pdf_path}. Error: {e}\")\n",
    "\n",
    "            return []\n",
    "        \n",
    "        \n",
    "\n",
    "        pages_and_texts = []\n",
    "        for page_number in tqdm(range(len(doc)), desc=\"Reading PDF pages\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text()\n",
    "            pages_and_texts.append({\"page_number\": page_number, \"text\": text})\n",
    "\n",
    "        logger.info(f\"Successfully read {len(pages_and_texts)} pages from {self.pdf_path}\")\n",
    "        return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Transformer to detect and convert bullet points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BulletPointTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to detect and convert bullet points into a structured format.\n",
    "    This ensures only successive bullet points are combined into one sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the BulletPointTransformer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method does nothing as the transformer doesn't require fitting.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data by identifying and formatting bullet points.\n",
    "        \n",
    "        :param X: List of documents or paragraphs to process\n",
    "        :return: List of documents with bullet points properly formatted\n",
    "        \"\"\"\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        transformed_data = []\n",
    "\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    # Apply bullet point transformation\n",
    "                    transformed_text = self._transform_bullet_points(text)\n",
    "                    item['formatted_text'] = transformed_text\n",
    "                    transformed_data.append(item)\n",
    "                else:\n",
    "                    logger.warning(f\"Missing 'formatted_text' key in item: {item}\")\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "        \n",
    "        return transformed_data\n",
    "\n",
    "    def _transform_bullet_points(self, text):\n",
    "        \"\"\"\n",
    "        Detect and combine only successive bullet points into a single sentence.\n",
    "        Bullet points can be identified by common characters such as *, -, or â€¢.\n",
    "        \n",
    "        :param text: The input text to process.\n",
    "        :return: Text with only successive bullet points combined into a single sentence.\n",
    "        \"\"\"\n",
    "        # Regular expression to match bullet points (handles *, -, or â€¢)\n",
    "        bullet_point_pattern = r'([*\\-â€¢])\\s?(.*?)(?=\\n|\\r|\\Z|\\s*$)'  # Match bullet points\n",
    "\n",
    "        # Match all bullet points\n",
    "        bullet_points = re.findall(bullet_point_pattern, text)\n",
    "\n",
    "        # If there are bullet points, combine only successive ones into a single sentence\n",
    "        if bullet_points:\n",
    "            # Iterate through the bullet points and combine only successive ones\n",
    "            combined_bullet_points = []\n",
    "            last_bullet = None\n",
    "            for bp in bullet_points:\n",
    "                if last_bullet is None:  # First bullet point\n",
    "                    combined_bullet_points.append(bp[1].strip())\n",
    "                else:  # Add only if successive bullet points\n",
    "                    combined_bullet_points.append(bp[1].strip())\n",
    "                last_bullet = bp\n",
    "            \n",
    "            # Join all successive bullet points into a single sentence\n",
    "            combined_bullet_points_sentence = \" \".join(combined_bullet_points) + \".\"\n",
    "            # Replace the bullet points section with the combined sentence\n",
    "            text = re.sub(bullet_point_pattern, \"\", text)  # Remove old bullet points\n",
    "            text = f\"{combined_bullet_points_sentence} {text}\"  # Add combined bullet points as a sentence\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceChunkerWithSummarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import spacy\n",
    "import logging\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceChunkerWithSummarization(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sentences_per_chunk=10, max_summary_length=500, num_beams=4):\n",
    "        \"\"\"\n",
    "        Initialize the SentenceChunkerWithSummarization.\n",
    "        \n",
    "        :param max_sentences_per_chunk: The maximum number of sentences per chunk.\n",
    "        :param max_summary_length: Maximum length of the generated summary.\n",
    "        :param num_beams: Number of beams for beam search during summary generation.\n",
    "        \"\"\"\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "        self.max_summary_length = max_summary_length\n",
    "        self.num_beams = num_beams\n",
    "        \n",
    "        # Load the SpaCy model and add the sentencizer\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "        \n",
    "        # Load the T5 model and tokenizer\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method does nothing as the model doesn't require fitting.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def generate_summary(self, text):\n",
    "        \"\"\"\n",
    "        Generate a summary for a given text using the T5 model.\n",
    "        \n",
    "        :param text: The input text to summarize\n",
    "        :return: The summarized text\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            logger.warning(\"Received invalid text input.\")\n",
    "            return \"Invalid input: Empty or non-string text\"\n",
    "\n",
    "        # Tokenize the input with the T5 summarization prompt\n",
    "        input_tokens = self.tokenizer.encode(\"summarize: \" + text, return_tensors='pt')\n",
    "\n",
    "        # Generate the summary using the model\n",
    "        # max_length=150,  # Increase the max_length for a longer summary\n",
    "        # min_length=50,   # Set a minimum length to prevent too short summaries\n",
    "        # num_beams=4,     # Use beam search for better quality summaries\n",
    "        # early_stopping=True,  # Stop early when the model is confident\n",
    "        # length_penalty=1.5,   # Apply a penalty to make sure summaries are not too short\n",
    "        output = self.model.generate(input_tokens, min_length=50, max_length=self.max_summary_length, num_beams=self.num_beams, early_stopping=True,  length_penalty=1.5)\n",
    "\n",
    "        # Decode the summary\n",
    "        summary = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def generate_unique_id(self, sentence_chunk):\n",
    "        \"\"\"\n",
    "        Generate a unique ID from a sentence chunk using SHA-256 hash.\n",
    "\n",
    "        :param sentence_chunk: The input sentence to generate the ID from.\n",
    "        :return: A unique ID (SHA-256 hash) as a hexadecimal string.\n",
    "        \"\"\"\n",
    "        # Step 1: Preprocess the sentence (optional, you could strip, lowercase, etc.)\n",
    "        processed_chunk = sentence_chunk.strip().lower()\n",
    "\n",
    "        # Step 2: Create the SHA-256 hash of the sentence\n",
    "        unique_id = hashlib.sha256(processed_chunk.encode()).hexdigest()\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data by chunking sentences and summarizing each chunk.\n",
    "        \n",
    "        :param X: List of documents or paragraphs to process\n",
    "        :return: List of dictionaries with sentence chunks and their summaries\n",
    "        \"\"\"\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        pages_and_chunks = []\n",
    "        sentences = []\n",
    "        pages = []\n",
    "\n",
    "        # Extract sentences and page numbers from the input\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item and 'page_number' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    page_number = item['page_number']\n",
    "                    if text:  # Check if text is not empty\n",
    "                        doc = self.nlp(text)  # Process text with SpaCy\n",
    "                        for sent in doc.sents:\n",
    "                            sentences.append(sent.text.strip())\n",
    "                            pages.append(page_number)\n",
    "                        logger.info(f\"Extracted sentences from page: {page_number}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty sentence found in item: {item}\")\n",
    "                else:\n",
    "                    logger.error(f\"Missing keys in item: {item}\")\n",
    "            elif isinstance(item, tuple) and len(item) == 2:\n",
    "                text = item[0].strip()\n",
    "                page_number = item[1]\n",
    "                doc = self.nlp(text)  # Process text with SpaCy\n",
    "                for sent in doc.sents:\n",
    "                    sentences.append(sent.text.strip())\n",
    "                    pages.append(page_number)\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "\n",
    "        # Organize sentences by page\n",
    "        sentences_by_page = {}\n",
    "        for sentence, page in zip(sentences, pages):\n",
    "            sentences_by_page.setdefault(page, []).append(sentence)\n",
    "\n",
    "        # Chunk sentences into fixed-size chunks and generate summaries\n",
    "        for page, sentences in sentences_by_page.items():\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            for i in range(0, len(sentences), self.max_sentences_per_chunk):\n",
    "                chunk_sentences = sentences[i:i + self.max_sentences_per_chunk]\n",
    "                chunk_text = \" \".join(chunk_sentences)\n",
    "                \n",
    "                # Generate the summary for the chunk of sentences\n",
    "                summary = self.generate_summary(chunk_text)\n",
    "\n",
    "                # Generate additional information\n",
    "                chunk_char_count = sum(len(s) for s in chunk_sentences)\n",
    "                chunk_word_count = sum(len(s.split()) for s in chunk_sentences)\n",
    "                chunk_token_count = sum(len(s) // 4 for s in chunk_sentences)\n",
    "                summary_char_count = len(summary)\n",
    "                summary_word_count = len(summary.split())\n",
    "\n",
    "                # Create a dictionary with both chunk data and summary data\n",
    "                chunk_dict = {\n",
    "                    \"sentence_chunk\": chunk_text,\n",
    "                    \"chunk_char_count\": chunk_char_count,\n",
    "                    \"chunk_word_count\": chunk_word_count,\n",
    "                    \"chunk_token_count\": chunk_token_count,\n",
    "                    \"page_number\": page,  # Include the page number\n",
    "                    \"summary_text\": summary,\n",
    "                    \"summary_char_count\": summary_char_count,\n",
    "                    \"summary_word_count\": summary_word_count,                    \n",
    "                    \"para_id\" : self.generate_unique_id(chunk_text)\n",
    "                }\n",
    "\n",
    "                # Only include chunks with more than 30 tokens\n",
    "                if chunk_token_count > 30:\n",
    "                    #logger.info(f\"Generated chunk and summary: {chunk_dict}\")\n",
    "                    pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "        #logger.info(f\"Processed {len(pages_and_chunks)} semantic chunks with summaries.\")\n",
    "        return pages_and_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuestionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class QuestionAnswerGenerator:\n",
    "    def __init__(self):\n",
    "        # Load Doc2Query model for question generation\n",
    "        self.model_name = 'doc2query/all-with_prefix-t5-base-v1'\n",
    "        self.qgen_tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.qgen_model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Load BERT or similar model for Question Answering (QA)\n",
    "        self.qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    def generate_questions(self, chunk, num_questions=5):\n",
    "        \"\"\"\n",
    "        Generate questions from a chunk of text using the Doc2Query model.\n",
    "\n",
    "        :param chunk: The input chunk of text to generate questions for.\n",
    "        :param num_questions: The number of questions to generate (default is 5).\n",
    "        :return: A list of generated questions.\n",
    "        \"\"\"\n",
    "        # Prepare the chunk for Doc2Query\n",
    "        input_text = f\"generate questions: {chunk}\"\n",
    "        inputs = self.qgen_tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "\n",
    "        # Check if we are using greedy decoding or beam search\n",
    "        if num_questions == 1:\n",
    "            # Use greedy decoding for one question\n",
    "            outputs = self.qgen_model.generate(\n",
    "                **inputs, \n",
    "                max_length=50, \n",
    "                num_return_sequences=1,  # Only generate one question\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        else:\n",
    "            # Use beam search for multiple questions\n",
    "            outputs = self.qgen_model.generate(\n",
    "                **inputs, \n",
    "                max_length=150, \n",
    "                num_return_sequences=num_questions,  # Generate multiple questions\n",
    "                num_beams=num_questions,  # Use beam search\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "\n",
    "        # Decode the generated questions\n",
    "        questions = [self.qgen_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        return questions\n",
    "\n",
    "    def generate_answers(self, chunk, questions):\n",
    "        \"\"\"\n",
    "        Generate answers for a list of questions given a chunk of text.\n",
    "\n",
    "        :param chunk: The input chunk of text for answering the questions.\n",
    "        :param questions: A list of questions to answer.\n",
    "        :return: A list of answers corresponding to the input questions.\n",
    "        \"\"\"\n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            # Encode the question and the context (chunk) for QA model with truncation and padding\n",
    "            inputs = self.qa_tokenizer.encode_plus(\n",
    "                question, \n",
    "                chunk, \n",
    "                return_tensors='pt', \n",
    "                truncation=True,  # Ensure the input sequence is truncated to fit the max length\n",
    "                padding=True,     # Pad the sequence if it's shorter than the maximum length\n",
    "                max_length=512    # Set the max length for the sequence\n",
    "            )\n",
    "\n",
    "            # Get the start and end positions of the answer\n",
    "            outputs = self.qa_model(**inputs)\n",
    "\n",
    "            # If the model outputs a tuple (start_scores, end_scores)\n",
    "            if isinstance(outputs, tuple):\n",
    "                answer_start_scores, answer_end_scores = outputs\n",
    "            else:\n",
    "                # If the model returns a dict, extract the start and end scores\n",
    "                answer_start_scores = outputs['start_logits']\n",
    "                answer_end_scores = outputs['end_logits']\n",
    "\n",
    "            # Get the most likely beginning and end of the answer\n",
    "            start_index = torch.argmax(answer_start_scores)\n",
    "            end_index = torch.argmax(answer_end_scores)\n",
    "\n",
    "            # Decode the answer from the token indices\n",
    "            answer = self.qa_tokenizer.convert_tokens_to_string(\n",
    "                self.qa_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1])\n",
    "            )\n",
    "\n",
    "            answers.append(answer)\n",
    "        return answers\n",
    "\n",
    "    def transform(self, chunk_data):\n",
    "        \"\"\"\n",
    "        Transform the input chunk data by generating questions and answers.\n",
    "\n",
    "        :param chunk_data: A list of chunks, each containing a sentence chunk.\n",
    "        :return: A list of chunks with generated questions and answers added.\n",
    "        \"\"\"\n",
    "        all_chunk_qa = []\n",
    "        for chunk in chunk_data:\n",
    "            chunk_text = chunk['sentence_chunk']  # Get the text of the chunk\n",
    "            questions = self.generate_questions(chunk_text)\n",
    "            answers = self.generate_answers(chunk_text, questions)\n",
    "            chunk['generated_questions'] = questions\n",
    "            chunk['generated_answers'] = answers\n",
    "            all_chunk_qa.append(chunk)\n",
    "        return all_chunk_qa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 3: Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sentences_per_chunk=5):\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "        # Load the SpaCy English model and add the sentencizer\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes=None):\n",
    "        pages_and_chunks = []\n",
    "        logger.info(f\"Input data for transformation: {X}\")\n",
    "        logger.info(f\"Input data length: {len(X)}\")\n",
    "\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        sentences = []\n",
    "        pages = []\n",
    "\n",
    "        # Extract sentences and page numbers\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item and 'page_number' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    page_number = item['page_number']\n",
    "                    if text:  # Check if text is not empty\n",
    "                        doc = self.nlp(text)  # Process text with SpaCy\n",
    "                        for sent in doc.sents:\n",
    "                            sentences.append(sent.text.strip())\n",
    "                            pages.append(page_number)\n",
    "                        #logger.info(f\"Extracted sentences from page: {page_number}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty sentence found in item: {item}\")\n",
    "                else:\n",
    "                    logger.error(f\"Missing keys in item: {item}\")\n",
    "            elif isinstance(item, tuple) and len(item) == 2:\n",
    "                text = item[0].strip()\n",
    "                page_number = item[1]\n",
    "                doc = self.nlp(text)  # Process text with SpaCy\n",
    "                for sent in doc.sents:\n",
    "                    sentences.append(sent.text.strip())\n",
    "                    pages.append(page_number)\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "\n",
    "        # Organize sentences by pages\n",
    "        sentences_by_page = {}\n",
    "        for sentence, page in zip(sentences, pages):\n",
    "            sentences_by_page.setdefault(page, []).append(sentence)\n",
    "\n",
    "        for page, sentences in sentences_by_page.items():\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            # Chunk sentences into fixed-size chunks\n",
    "            for i in range(0, len(sentences), self.max_sentences_per_chunk):\n",
    "                chunk_sentences = sentences[i:i + self.max_sentences_per_chunk]\n",
    "                chunk_token_count = sum(len(s) // 4 for s in chunk_sentences)\n",
    "                chunk_dict = {\n",
    "                    \"sentence_chunk\": \" \".join(chunk_sentences),\n",
    "                    \"chunk_char_count\": sum(len(s) for s in chunk_sentences),\n",
    "                    \"chunk_word_count\": sum(len(s.split()) for s in chunk_sentences),\n",
    "                    \"chunk_token_count\": sum(len(s) // 4 for s in chunk_sentences),  # Adjust if needed\n",
    "                    \"page_number\": page  # Include the page number\n",
    "                }\n",
    "                if chunk_token_count > 30:\n",
    "                    logger.info(f\"Generated chunk: {chunk_dict}\")\n",
    "                    pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "        logger.info(f\"Processed {len(pages_and_chunks)} semantic chunks.\")\n",
    "        return pages_and_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes):\n",
    "        sentences = [chunk[\"sentence_chunk\"] for chunk in X]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        \n",
    "        for i, chunk in enumerate(X):\n",
    "            chunk[\"embedding\"] = embeddings[i]\n",
    "        \n",
    "        #log_output(\"Embedding Generator: \"+ len(X))  # Log output\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Save to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBSaver(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, chroma_db_dir=\"chroma_db_dir\"):  # Ensure this points to your local ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "        self.collection = self.client.get_or_create_collection(\"pdf_chunks\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes):\n",
    "        i = 0 \n",
    "        for chunk, doc_attr in zip(X, document_attributes):\n",
    "\n",
    "            document_id = f\"{doc_attr['make']}_{doc_attr['model']}_{doc_attr['year']}_{doc_attr['style']}\"\n",
    "            \n",
    "            # Log the chunk being added\n",
    "            text =  chunk[\"sentence_chunk\"]\n",
    "            chunk_char_count = chunk[\"chunk_char_count\"]\n",
    "            chunk_word_count = chunk[\"chunk_word_count\"]\n",
    "            if chunk[\"sentence_chunk\"].strip():  # Ensure it's not empty\n",
    "                #print(f\"Adding document ID: {document_id}, Content: '{chunk['sentence_chunk']}'\")\n",
    "                \n",
    "                self.collection.add(\n",
    "                    documents=[text],\n",
    "                    embeddings=[chunk[\"embedding\"].tolist()],\n",
    "                    metadatas=[{\"source\": document_id}],\n",
    "                    ids = [f\"{document_id}_{chunk['page_number']}_{chunk_word_count}_{chunk_char_count}\"]\n",
    "\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping empty document for ID: {document_id}\")\n",
    "            i=i+1\n",
    "\n",
    "        log_output(\"ChromaDB Saver , Data saved to ChromaDB\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('pdf_reader', PDFReader),  # Step 1: Read PDF (pass the class, not an instance)\n",
    "        ('text_formatter', TextFormatter()),  # Step 2: Format text\n",
    "        ('bullet_point_transformer', BulletPointTransformer()),  # Step 3: Transform bullet points\n",
    "        ('sentence_chunker', SentenceChunkerWithSummarization()),  # Step 4: Chunk sentences\n",
    "        ('question_answer_generator', QuestionAnswerGenerator()),  # Step 5: Generate QA pairs (call the class)\n",
    "        ('embedding_generator', EmbeddingGenerator()),  # Step 6: Generate embeddings\n",
    "        ('chromadb_saver', ChromaDBSaver())  # Step 7: Save to ChromaDB\n",
    "    ])\n",
    "\n",
    "    # Create a document ID based on attributes\n",
    "    document_id = f\"{document['make']}_{document['model']}_{document['year']}_{document['style']}\"\n",
    "\n",
    "    # Instantiate PDFReader manually, as it requires the file path\n",
    "    pdf_reader = PDFReader(document.get('pdf_path'))\n",
    "    result = pdf_reader.fit_transform(document.get('pdf_path'))  # Read the PDF file\n",
    "\n",
    "    # Process the document through each pipeline step\n",
    "    result = pipeline.named_steps['text_formatter'].transform(result)\n",
    "    result = pipeline.named_steps['bullet_point_transformer'].transform(result)\n",
    "    result = pipeline.named_steps['sentence_chunker'].transform(result)\n",
    "    result = pipeline.named_steps['question_answer_generator'].transform(result)  # Generate questions and answers\n",
    "\n",
    "    # Generate embeddings and add them to the result\n",
    "    embeddings = pipeline.named_steps['embedding_generator'].transform(result, document)\n",
    "\n",
    "    # Save the embeddings and document data to ChromaDB\n",
    "    pipeline.named_steps['chromadb_saver'].transform(embeddings, [document] * len(embeddings))\n",
    "\n",
    "    # Process each chunk and add the data to the list\n",
    "    all_chunk_data = []\n",
    "    all_QandA =[]\n",
    "    for chunk in result:\n",
    "        chunk_data = {\n",
    "            \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "            \"chunk_char_count\": chunk[\"chunk_char_count\"],\n",
    "            \"chunk_word_count\": chunk[\"chunk_word_count\"],\n",
    "            \"chunk_token_count\": chunk[\"chunk_token_count\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"summary_text\": chunk[\"summary_text\"],\n",
    "            \"summary_char_count\": chunk[\"summary_char_count\"],\n",
    "            \"summary_word_count\": chunk[\"summary_word_count\"],\n",
    "            \"para_id\" : chunk[\"para_id\"],               \n",
    "        }\n",
    "        for index, question in enumerate(chunk[\"generated_questions\"], 0):\n",
    "            qa_data  = {\n",
    "               \n",
    "               \"page_number\": chunk[\"page_number\"],\n",
    "               \"para_id\" : chunk[\"para_id\"],   \n",
    "               \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "               \"question\" : question,\n",
    "               \"answer\" :  chunk[\"generated_answers\"][index]\n",
    "            }\n",
    "            all_QandA.append(qa_data)\n",
    "  \n",
    "        all_chunk_data.append(chunk_data)\n",
    "\n",
    "    # Convert the list of chunks into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_chunk_data)\n",
    "    df_qa = pd.DataFrame(all_QandA)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"manuals/\" + document_id + \".csv\", index=False)\n",
    "    df_qa.to_csv(\"manuals/\" + document_id + \"_QA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cffd8cade74a668cd50bb9fb629573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read 97 pages from manuals/FragglesX500FMS-2024-V4.pdf\n",
      "INFO:__main__:Extracted sentences from page: 0\n",
      "INFO:__main__:Extracted sentences from page: 1\n",
      "INFO:__main__:Extracted sentences from page: 2\n",
      "INFO:__main__:Extracted sentences from page: 3\n",
      "INFO:__main__:Extracted sentences from page: 4\n",
      "INFO:__main__:Extracted sentences from page: 5\n",
      "INFO:__main__:Extracted sentences from page: 6\n",
      "INFO:__main__:Extracted sentences from page: 7\n",
      "INFO:__main__:Extracted sentences from page: 8\n",
      "INFO:__main__:Extracted sentences from page: 9\n",
      "INFO:__main__:Extracted sentences from page: 10\n",
      "INFO:__main__:Extracted sentences from page: 11\n",
      "INFO:__main__:Extracted sentences from page: 12\n",
      "INFO:__main__:Extracted sentences from page: 13\n",
      "INFO:__main__:Extracted sentences from page: 14\n",
      "INFO:__main__:Extracted sentences from page: 15\n",
      "INFO:__main__:Extracted sentences from page: 16\n",
      "INFO:__main__:Extracted sentences from page: 17\n",
      "INFO:__main__:Extracted sentences from page: 18\n",
      "INFO:__main__:Extracted sentences from page: 19\n",
      "INFO:__main__:Extracted sentences from page: 20\n",
      "INFO:__main__:Extracted sentences from page: 21\n",
      "INFO:__main__:Extracted sentences from page: 22\n",
      "INFO:__main__:Extracted sentences from page: 23\n",
      "INFO:__main__:Extracted sentences from page: 24\n",
      "INFO:__main__:Extracted sentences from page: 25\n",
      "INFO:__main__:Extracted sentences from page: 26\n",
      "INFO:__main__:Extracted sentences from page: 27\n",
      "INFO:__main__:Extracted sentences from page: 28\n",
      "INFO:__main__:Extracted sentences from page: 29\n",
      "INFO:__main__:Extracted sentences from page: 30\n",
      "INFO:__main__:Extracted sentences from page: 31\n",
      "INFO:__main__:Extracted sentences from page: 32\n",
      "INFO:__main__:Extracted sentences from page: 33\n",
      "INFO:__main__:Extracted sentences from page: 34\n",
      "INFO:__main__:Extracted sentences from page: 35\n",
      "INFO:__main__:Extracted sentences from page: 36\n",
      "INFO:__main__:Extracted sentences from page: 37\n",
      "INFO:__main__:Extracted sentences from page: 38\n",
      "INFO:__main__:Extracted sentences from page: 39\n",
      "INFO:__main__:Extracted sentences from page: 40\n",
      "INFO:__main__:Extracted sentences from page: 41\n",
      "INFO:__main__:Extracted sentences from page: 42\n",
      "INFO:__main__:Extracted sentences from page: 43\n",
      "INFO:__main__:Extracted sentences from page: 44\n",
      "INFO:__main__:Extracted sentences from page: 45\n",
      "INFO:__main__:Extracted sentences from page: 46\n",
      "INFO:__main__:Extracted sentences from page: 47\n",
      "INFO:__main__:Extracted sentences from page: 48\n",
      "INFO:__main__:Extracted sentences from page: 49\n",
      "INFO:__main__:Extracted sentences from page: 50\n",
      "INFO:__main__:Extracted sentences from page: 51\n",
      "INFO:__main__:Extracted sentences from page: 52\n",
      "INFO:__main__:Extracted sentences from page: 53\n",
      "INFO:__main__:Extracted sentences from page: 54\n",
      "INFO:__main__:Extracted sentences from page: 55\n",
      "INFO:__main__:Extracted sentences from page: 56\n",
      "INFO:__main__:Extracted sentences from page: 57\n",
      "INFO:__main__:Extracted sentences from page: 58\n",
      "INFO:__main__:Extracted sentences from page: 59\n",
      "INFO:__main__:Extracted sentences from page: 60\n",
      "INFO:__main__:Extracted sentences from page: 61\n",
      "INFO:__main__:Extracted sentences from page: 62\n",
      "INFO:__main__:Extracted sentences from page: 63\n",
      "INFO:__main__:Extracted sentences from page: 64\n",
      "INFO:__main__:Extracted sentences from page: 65\n",
      "INFO:__main__:Extracted sentences from page: 66\n",
      "INFO:__main__:Extracted sentences from page: 67\n",
      "INFO:__main__:Extracted sentences from page: 68\n",
      "INFO:__main__:Extracted sentences from page: 69\n",
      "INFO:__main__:Extracted sentences from page: 70\n",
      "INFO:__main__:Extracted sentences from page: 71\n",
      "INFO:__main__:Extracted sentences from page: 72\n",
      "INFO:__main__:Extracted sentences from page: 73\n",
      "INFO:__main__:Extracted sentences from page: 74\n",
      "INFO:__main__:Extracted sentences from page: 75\n",
      "INFO:__main__:Extracted sentences from page: 76\n",
      "INFO:__main__:Extracted sentences from page: 77\n",
      "INFO:__main__:Extracted sentences from page: 78\n",
      "INFO:__main__:Extracted sentences from page: 79\n",
      "INFO:__main__:Extracted sentences from page: 80\n",
      "INFO:__main__:Extracted sentences from page: 81\n",
      "INFO:__main__:Extracted sentences from page: 82\n",
      "INFO:__main__:Extracted sentences from page: 83\n",
      "INFO:__main__:Extracted sentences from page: 84\n",
      "INFO:__main__:Extracted sentences from page: 85\n",
      "INFO:__main__:Extracted sentences from page: 86\n",
      "INFO:__main__:Extracted sentences from page: 87\n",
      "INFO:__main__:Extracted sentences from page: 88\n",
      "INFO:__main__:Extracted sentences from page: 89\n",
      "INFO:__main__:Extracted sentences from page: 90\n",
      "INFO:__main__:Extracted sentences from page: 91\n",
      "INFO:__main__:Extracted sentences from page: 92\n",
      "INFO:__main__:Extracted sentences from page: 93\n",
      "INFO:__main__:Extracted sentences from page: 94\n",
      "INFO:__main__:Extracted sentences from page: 95\n",
      "INFO:__main__:Extracted sentences from page: 96\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408c754656da4271ac656a09f6e1ade9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b28d65eece48dd977d6d820cde38d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read 28 pages from manuals/FragglesX700HCM-2022-V2.pdf\n",
      "INFO:__main__:Extracted sentences from page: 0\n",
      "INFO:__main__:Extracted sentences from page: 1\n",
      "INFO:__main__:Extracted sentences from page: 2\n",
      "INFO:__main__:Extracted sentences from page: 3\n",
      "INFO:__main__:Extracted sentences from page: 4\n",
      "INFO:__main__:Extracted sentences from page: 5\n",
      "INFO:__main__:Extracted sentences from page: 6\n",
      "INFO:__main__:Extracted sentences from page: 7\n",
      "INFO:__main__:Extracted sentences from page: 8\n",
      "INFO:__main__:Extracted sentences from page: 9\n",
      "INFO:__main__:Extracted sentences from page: 10\n",
      "INFO:__main__:Extracted sentences from page: 11\n",
      "INFO:__main__:Extracted sentences from page: 12\n",
      "INFO:__main__:Extracted sentences from page: 13\n",
      "INFO:__main__:Extracted sentences from page: 14\n",
      "INFO:__main__:Extracted sentences from page: 15\n",
      "INFO:__main__:Extracted sentences from page: 16\n",
      "INFO:__main__:Extracted sentences from page: 17\n",
      "INFO:__main__:Extracted sentences from page: 18\n",
      "INFO:__main__:Extracted sentences from page: 19\n",
      "INFO:__main__:Extracted sentences from page: 20\n",
      "INFO:__main__:Extracted sentences from page: 21\n",
      "INFO:__main__:Extracted sentences from page: 22\n",
      "INFO:__main__:Extracted sentences from page: 23\n",
      "INFO:__main__:Extracted sentences from page: 24\n",
      "INFO:__main__:Extracted sentences from page: 25\n",
      "INFO:__main__:Extracted sentences from page: 26\n",
      "INFO:__main__:Extracted sentences from page: 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a5934f66274e1d9c36988690db5525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Pipeline Execution\n",
    "input_documents = [\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X500\",\n",
    "        \"year\": \"2024\",\n",
    "        \"style\": \"FMS\",\n",
    "        \"pdf_path\": \"manuals/FragglesX500FMS-2024-V4.pdf\"  \n",
    "    },\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X700\",\n",
    "        \"year\": \"2022\",\n",
    "        \"style\": \"HCM\",\n",
    "        \"pdf_path\": \"manuals/FragglesX700HCM-2022-V2.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each document\n",
    "for doc in input_documents:\n",
    "    process_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9c3eb058924a6cb2143aee5b24c001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['â€¢  Traction control may be off, or the vehicle might be  in reverse (R). Ensure the vehicle is moving forward. Q: Why doesnâ€™t the system offer a parking space? â€¢  Sensors could be blocked or damaged, or there may  not be enough space for the vehicle to park safely. Q: Why isnâ€™t the vehicle correctly positioned in the  parking space? â€¢  The system may struggle with irregular curbs, high  attachments on nearby vehicles, or if the parking  space has changed after the vehicle has already  passed. Important Precautions  â€¢  Always Remain in the Vehicle: You must stay in  control of the vehicle and be ready to intervene if  necessary.. FragglesX500FMS 2024          49  8. Release the brake pedal and allow the system to  maneuver the vehicle. 9. The vehicle will back into the parking space and shift  into park (P) when complete.',\n",
       "  'Tap the Active Park Assist icon on the touchscreen. 3. Select Perpendicular Parking mode. 4. Use the turn signal to indicate the side you want to  park. 5. Drive about 3 ft (1 m) away from parked vehicles. 6. The system detects the space; press and hold the  brake pedal. 7.',\n",
       "  'Shift into neutral (N), then press and hold the  Parking Aid button. 8. Release the brake pedal and let the system control  the vehicle as it parks. 9. When the parking maneuver is complete, the vehicle  shifts into park (P). Tips:  â€¢  You can slow down the vehicle at any time by  pressing the brake pedal. â€¢  The system parks closer to the object in front for  easier access to the trunk. Entering a Perpendicular Parking Space  1. Press the Parking Aid button. 2.',\n",
       "  'Press the Active Park Assist icon on the  touchscreen to enable the system. 3. Choose the desired parking mode:  o Parallel Park In  o Perpendicular Park In  o Parallel Park Out  Cancelling Active Park Assist:  â€¢  Shift out of neutral (N) to cancel the system. â€¢  To pause, release the parking aid button. Resuming Active Park Assist:  â€¢  Press and hold the parking aid button again. Entering a Parallel Parking Space  1. Press the Parking Aid button. 2. Tap the Active Park Assist icon on the touchscreen. 3.',\n",
       "  'Use the turn signal to indicate the side of the vehicle  where you want to park. o If no turn signal is used, the system defaults  to searching on the passenger side. 4. Drive your vehicle about 3 ft (1 m) away from parked  vehicles. 5. Wait for the system to detect a suitable parking  space. A tone sounds and a message appears. 6. Press and hold the brake pedal. 7.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ChromaDBSearcher:\n",
    "    def __init__(self, chroma_db_dir=\"chroma_db_dir\", model_name=\"all-mpnet-base-v2\"):\n",
    "        self.client = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "        self.collection = self.client.get_collection(\"pdf_chunks\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def search_by_id(self, document_id, query):\n",
    "        try:\n",
    "            query_embedding = self.model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
    "            results = self.collection.query(\n",
    "                query_embedding.tolist(),\n",
    "                where={\"source\": document_source},\n",
    "                n_results=5\n",
    "            )\n",
    "            \n",
    "            if results and results['documents']:\n",
    "                document_content = results['documents'][0]  # Remove extra spaces\n",
    "                if document_content:\n",
    "                    return results['documents']\n",
    "                else:\n",
    "                    print(\"Document content is empty.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"No documents found.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search by ID: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "searcher = ChromaDBSearcher()\n",
    "document_source = \"Fraggles_X500_2024_FMS\"  # Replace with the actual document ID you want to search for\n",
    "#document_source = \"Ford_Mustang_2023_MACH-E\"\n",
    "query = \"how to use parking breakes?\"  # Replace with the query you want to search for\n",
    "\n",
    "searcher.search_by_id(document_source, query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
