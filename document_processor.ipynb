{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "import re  # For regex operations\n",
    "import logging\n",
    "\n",
    "# Natural Language Processing (NLP) and Embeddings\n",
    "import spacy\n",
    "\n",
    "# Machine Learning Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Custome pipelines classes\n",
    "from pipeline.chroma_db import ChromaDBSaver\n",
    "from pipeline.pdf_reader import PDFReader\n",
    "from pipeline.text_proccessor import TextFormatter\n",
    "from pipeline.chunk_proccessor import SentenceChunkerWithSummarization\n",
    "from pipeline.question_generator import QuestionAnswerGenerator\n",
    "from pipeline.embedding_proccessor import EmbeddingGenerator\n",
    "\n",
    "#ChromaDBSearcher\n",
    "from common.chroma_db import ChromaDBSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_output(string):\n",
    "    #logger.info(string)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing Pipeline\r\n",
    "\r\n",
    "This cell processes a document through a series of transformations and generates embeddings, QA pairs, and chunk-level data. The final output is saved to a CSV file and stored in ChromaDB.\r\n",
    "\r\n",
    "1. **Pipeline Steps**:\r\n",
    "   - The `process_document()` function orchestrates a pipeline of transformations on a PDF document.\r\n",
    "   - The pipeline includes several steps that are applied sequentially to the document:\r\n",
    "     1. **PDFReader**: Reads the PDF document from the specified file path.\r\n",
    "     2. **TextFormatter**: Formats the text extracted from the PDF (e.g., removing unwanted characters or formatting).\r\n",
    "     3. **SentenceChunkerWithSummarization**: Divides the document into chunks and summarizes the content.\r\n",
    "     4. **QuestionAnswerGenerator**: Generates questions and corresponding answers for each chunk of text.\r\n",
    "     5. **EmbeddingGenerator**: Generates embeddings for the text using a pre-trained model.\r\n",
    "     6. **ChromaDBSaver**: Saves the embeddings and document data into a ChromaDB collection.\r\n",
    "\r\n",
    "2. **Document Processing**:\r\n",
    "   - A document ID is created based on the document's attributes (`make`, `model`, `year`, `style`).\r\n",
    "   - The `PDFReader` class is instantiated manually, as it requires the file path to read the PDF.\r\n",
    "   - The document is processed through the pipeline, where the text is formatted, chunked, questions are generated, and embeddings are created.\r\n",
    "\r\n",
    "3. **Saving Results**:\r\n",
    "   - The embeddings are saved to ChromaDB for further retrieval and analysis.\r\n",
    "   - The document's chunk data and corresponding generated questions and answers are extracted into separate lists (`all_chunk_data` and `all_QandA`).\r\n",
    "   - The chunk data and QA pairs are stored in pandas DataFrames.\r\n",
    "\r\n",
    "4. **Exporting to CSV**:\r\n",
    "   - The chunk-level data is saved to a CSV file, with the name based on the document ID (`document_id + \".csv\"`).\r\n",
    "   - The generated QA pairs are saved to a separate CSV file (`document_id + \"_QA.csv\"`).\r\n",
    "\r\n",
    "This pipeline allows for automated document processing, transforming raw PDF data into structured and searchable information, which can then be used for various retrieval and analysis tasks.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('pdf_reader', PDFReader),  # Step 1: Read PDF (pass the class, not an instance)\n",
    "        ('text_formatter', TextFormatter()),  # Step 2: Format text\n",
    "        #('bullet_point_transformer', BulletPointTransformer()),  # Step 3: Transform bullet points\n",
    "        ('sentence_chunker', SentenceChunkerWithSummarization()),  # Step 4: Chunk sentences\n",
    "        ('question_answer_generator', QuestionAnswerGenerator()),  # Step 5: Generate QA pairs (call the class)\n",
    "        ('embedding_generator', EmbeddingGenerator()),  # Step 6: Generate embeddings\n",
    "        ('chromadb_saver', ChromaDBSaver())  # Step 7: Save to ChromaDB\n",
    "    ])\n",
    "\n",
    "    # Create a document ID based on attributes\n",
    "    document_id = f\"{document['make']}_{document['model']}_{document['year']}_{document['style']}\"\n",
    "\n",
    "    # Instantiate PDFReader manually, as it requires the file path\n",
    "    pdf_reader = PDFReader(document.get('pdf_path'), logger)\n",
    "    result = pdf_reader.fit_transform(document.get('pdf_path'))  # Read the PDF file\n",
    "\n",
    "    # Process the document through each pipeline step\n",
    "    result = pipeline.named_steps['text_formatter'].transform(result)\n",
    "    #result = pipeline.named_steps['bullet_point_transformer'].transform(result)\n",
    "    result = pipeline.named_steps['sentence_chunker'].transform(result)\n",
    "    result = pipeline.named_steps['question_answer_generator'].transform(result)  # Generate questions and answers\n",
    "\n",
    "    # Generate embeddings and add them to the result\n",
    "    embeddings = pipeline.named_steps['embedding_generator'].transform(result, document)\n",
    "\n",
    "    # Save the embeddings and document data to ChromaDB\n",
    "    pipeline.named_steps['chromadb_saver'].transform(embeddings, [document] * len(embeddings))\n",
    "\n",
    "    # Process each chunk and add the data to the list\n",
    "    all_chunk_data = []\n",
    "    all_QandA =[]\n",
    "    for chunk in result:\n",
    "        chunk_data = {\n",
    "            \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "            \"chunk_char_count\": chunk[\"chunk_char_count\"],\n",
    "            \"chunk_word_count\": chunk[\"chunk_word_count\"],\n",
    "            \"chunk_token_count\": chunk[\"chunk_token_count\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"summary_text\": chunk[\"summary_text\"],\n",
    "            \"summary_char_count\": chunk[\"summary_char_count\"],\n",
    "            \"summary_word_count\": chunk[\"summary_word_count\"],\n",
    "            \"para_id\" : chunk[\"para_id\"],               \n",
    "        }\n",
    "        for index, question in enumerate(chunk[\"generated_questions\"], 0):\n",
    "            qa_data  = {\n",
    "               \n",
    "               \"page_number\": chunk[\"page_number\"],\n",
    "               \"para_id\" : chunk[\"para_id\"],   \n",
    "               \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "               \"question\" : question,\n",
    "               \"answer\" :  chunk[\"generated_answers\"][index]\n",
    "            }\n",
    "            all_QandA.append(qa_data)\n",
    "  \n",
    "        all_chunk_data.append(chunk_data)\n",
    "\n",
    "    # Convert the list of chunks into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_chunk_data)\n",
    "    df_qa = pd.DataFrame(all_QandA)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"manuals/\" + document_id + \".csv\", index=False)\n",
    "    df_qa.to_csv(\"manuals/\" + document_id + \"_QA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline Execution: Processing Multiple Documents\r\n",
    "\r\n",
    "This cell runs the document processing pipeline on a list of input documents. It demonstrates how to apply the pipeline to multiple documents in sequence.\r\n",
    "\r\n",
    "1. **Input Documents**:\r\n",
    "   - A list of dictionaries is defined, where each dictionary contains metadata for a document:\r\n",
    "     - `make`, `model`, `year`, and `style`: These are attributes of the document (e.g., the make and model of a vehicle or product).\r\n",
    "     - `pdf_path`: The file path to the PDF document that will be processed.\r\n",
    "   \r\n",
    "   Two example documents are provided:\r\n",
    "   - \"Fraggles X500 2024 FMS\"\r\n",
    "   - \"Fraggles X700 2022 HCM\"\r\n",
    "\r\n",
    "2. **Processing Each Document**:\r\n",
    "   - The code loops through each document in the `input_documents` list.\r\n",
    "   - For each document, the `process_document()` function is called, which processes the document using the predefined pipeline (as described in the previous markdown explanation).\r\n",
    "   - This includes reading the PDF, extracting and formatting text, chunking the text, generating question-answer pairs, creating embeddings, and saving results to ChromaDB and CSV files.\r\n",
    "\r\n",
    "By using this loop, you can easily process multiple documents in batch, allowing for scalable processing and storage of information for various documents in the collection.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381d73b680d345109fa6efeabf7e61be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read 97 pages from manuals/FragglesX500FMS-2024-V4.pdf\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 0\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 1\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 2\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 3\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 4\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 5\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 6\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 7\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 8\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 9\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 10\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 11\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 12\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 13\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 14\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 15\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 16\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 17\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 18\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 19\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 20\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 21\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 22\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 23\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 24\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 25\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 26\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 27\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 28\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 29\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 30\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 31\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 32\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 33\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 34\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 35\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 36\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 37\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 38\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 39\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 40\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 41\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 42\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 43\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 44\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 45\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 46\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 47\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 48\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 49\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 50\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 51\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 52\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 53\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 54\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 55\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 56\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 57\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 58\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 59\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 60\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 61\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 62\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 63\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 64\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 65\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 66\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 67\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 68\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 69\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 70\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 71\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 72\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 73\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 74\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 75\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 76\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 77\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 78\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 79\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 80\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 81\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 82\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 83\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 84\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 85\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 86\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 87\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 88\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 89\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 90\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 91\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 92\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 93\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 94\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 95\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 96\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d624b8584247189c8b5a544b54686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619518a373d743b8b9abff545d5e213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read 28 pages from manuals/FragglesX700HCM-2022-V2.pdf\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 0\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 1\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 2\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 3\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 4\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 5\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 6\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 7\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 8\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 9\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 10\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 11\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 12\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 13\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 14\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 15\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 16\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 17\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 18\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 19\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 20\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 21\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 22\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 23\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 24\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 25\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 26\n",
      "INFO:pipeline.chunk_proccessor:Extracted sentences from page: 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a261e6d33e42148f8bf5898ad433a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "input_documents = [\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X500\",\n",
    "        \"year\": \"2024\",\n",
    "        \"style\": \"FMS\",\n",
    "        \"pdf_path\": \"manuals/FragglesX500FMS-2024-V4.pdf\"  \n",
    "    },\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X700\",\n",
    "        \"year\": \"2022\",\n",
    "        \"style\": \"HCM\",\n",
    "        \"pdf_path\": \"manuals/FragglesX700HCM-2022-V2.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each document\n",
    "for doc in input_documents:\n",
    "    process_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage: Searching with ChromaDB\n",
    "\n",
    "This cell demonstrates how to use the `ChromaDBSearcher` class to search for relevant context within a document using a query.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - A `ChromaDBSearcher` object (`searcher`) is instantiated. This object will interact with a ChromaDB collection to retrieve relevant document chunks.\n",
    "   \n",
    "2. **Setting the Document Source**:\n",
    "   - The variable `document_source` is set to `\"Fraggles_X500_2024_FMS\"`, which is the document ID you wish to search within.\n",
    "   - You can replace this with any other document ID (e.g., `\"Ford_Mustang_2023_MACH-E\"`) depending on the document you are interested in.\n",
    "\n",
    "3. **Defining the Query**:\n",
    "   - The `query` variable contains the text string `\"how to use parking breakes?\"`, which will be used to search for relevant answers within the specified document.\n",
    "   - This is the search term or question for which you want to find relevant content from the document.\n",
    "\n",
    "4. **Performing the Search**:\n",
    "   - The `search_by_id()` method of `ChromaDBSearcher` is called with the `document_source` and `query` as arguments.\n",
    "   - This method will return the top results (up to 10 by default) based on the relevance of the query and the document chunks stored in ChromaDB.\n",
    "\n",
    "This demonstrates how you can use ChromaDB to search for specific information in a document based on a query. It retrieves relevant text chunks that may provide an answer or context to the question posed in the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a55e3a79954ceba20d1e7ea0e04f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"['FragglesX500FMS 2024          49  8. Release the brake pedal and allow the system to  maneuver the vehicle. 9. The vehicle will back into the parking space and shift  into park (P) when complete. Note: The system centers the vehicle between objects, not  based on parking lines. Exiting a Parallel Parking Space  1. Press the Parking Aid button. 2. Tap the Active Park Assist icon on the touchscreen. 3.', 'Select Parallel Park Exit mode. 4. Use the turn signal to choose the direction for exit. 5. Press and hold the brake pedal. 6. Shift into neutral (N), then press and hold the  Parking Aid button. 7. Release the parking brake and let the system  maneuver the vehicle out of the space. 8.', 'Tap the Active Park Assist icon on the touchscreen. 3. Select Perpendicular Parking mode. 4. Use the turn signal to indicate the side you want to  park. 5. Drive about 3 ft (1 m) away from parked vehicles. 6. The system detects the space; press and hold the  brake pedal. 7.']\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "searcher = ChromaDBSearcher()\n",
    "document_source = \"Fraggles_X500_2024_FMS\"  # Replace with the actual document ID you want to search for\n",
    "#document_source = \"Ford_Mustang_2023_MACH-E\"\n",
    "query = \"how to use parking breakes?\"  # Replace with the query you want to search for\n",
    "\n",
    "searcher.search_by_id(document_source, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 22:31:28 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             36W /  250W |    1751MiB /  16384MiB |      1%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-16GB           On  |   00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             38W /  250W |       3MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    965227      C   ...n3.11-anaconda/2024.02-1/bin/python       1748MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
