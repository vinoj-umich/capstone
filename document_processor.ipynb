{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "import re  # For regex operations\n",
    "import logging\n",
    "\n",
    "# Natural Language Processing (NLP) and Embeddings\n",
    "import spacy\n",
    "\n",
    "# Machine Learning Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Custome pipelines classes\n",
    "from pipeline.chroma_db import ChromaDBSaver\n",
    "from pipeline.pdf_reader import PDFReader\n",
    "from pipeline.text_proccessor import TextFormatter\n",
    "from pipeline.chunk_proccessor import SentenceChunkerWithSummarization\n",
    "from pipeline.question_generator import QuestionAnswerGenerator\n",
    "from pipeline.embedding_proccessor import EmbeddingGenerator\n",
    "\n",
    "#ChromaDBSearcher\n",
    "from common.chroma_db import ChromaDBSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_output(string):\n",
    "    #logger.info(string)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing Pipeline\r\n",
    "\r\n",
    "This cell processes a document through a series of transformations and generates embeddings, QA pairs, and chunk-level data. The final output is saved to a CSV file and stored in ChromaDB.\r\n",
    "\r\n",
    "1. **Pipeline Steps**:\r\n",
    "   - The `process_document()` function orchestrates a pipeline of transformations on a PDF document.\r\n",
    "   - The pipeline includes several steps that are applied sequentially to the document:\r\n",
    "     1. **PDFReader**: Reads the PDF document from the specified file path.\r\n",
    "     2. **TextFormatter**: Formats the text extracted from the PDF (e.g., removing unwanted characters or formatting).\r\n",
    "     3. **SentenceChunkerWithSummarization**: Divides the document into chunks and summarizes the content.\r\n",
    "     4. **QuestionAnswerGenerator**: Generates questions and corresponding answers for each chunk of text.\r\n",
    "     5. **EmbeddingGenerator**: Generates embeddings for the text using a pre-trained model.\r\n",
    "     6. **ChromaDBSaver**: Saves the embeddings and document data into a ChromaDB collection.\r\n",
    "\r\n",
    "2. **Document Processing**:\r\n",
    "   - A document ID is created based on the document's attributes (`make`, `model`, `year`, `style`).\r\n",
    "   - The `PDFReader` class is instantiated manually, as it requires the file path to read the PDF.\r\n",
    "   - The document is processed through the pipeline, where the text is formatted, chunked, questions are generated, and embeddings are created.\r\n",
    "\r\n",
    "3. **Saving Results**:\r\n",
    "   - The embeddings are saved to ChromaDB for further retrieval and analysis.\r\n",
    "   - The document's chunk data and corresponding generated questions and answers are extracted into separate lists (`all_chunk_data` and `all_QandA`).\r\n",
    "   - The chunk data and QA pairs are stored in pandas DataFrames.\r\n",
    "\r\n",
    "4. **Exporting to CSV**:\r\n",
    "   - The chunk-level data is saved to a CSV file, with the name based on the document ID (`document_id + \".csv\"`).\r\n",
    "   - The generated QA pairs are saved to a separate CSV file (`document_id + \"_QA.csv\"`).\r\n",
    "\r\n",
    "This pipeline allows for automated document processing, transforming raw PDF data into structured and searchable information, which can then be used for various retrieval and analysis tasks.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('pdf_reader', PDFReader),  # Step 1: Read PDF (pass the class, not an instance)\n",
    "        ('text_formatter', TextFormatter()),  # Step 2: Format text\n",
    "        #('bullet_point_transformer', BulletPointTransformer()),  # Step 3: Transform bullet points\n",
    "        ('sentence_chunker', SentenceChunkerWithSummarization()),  # Step 4: Chunk sentences\n",
    "        ('question_answer_generator', QuestionAnswerGenerator()),  # Step 5: Generate QA pairs (call the class)\n",
    "        ('embedding_generator', EmbeddingGenerator()),  # Step 6: Generate embeddings\n",
    "        ('chromadb_saver', ChromaDBSaver())  # Step 7: Save to ChromaDB\n",
    "    ])\n",
    "\n",
    "    # Create a document ID based on attributes\n",
    "    document_id = f\"{document['make']}_{document['model']}_{document['year']}_{document['style']}\"\n",
    "\n",
    "    # Instantiate PDFReader manually, as it requires the file path\n",
    "    pdf_reader = PDFReader(document.get('pdf_path'), logger)\n",
    "    result = pdf_reader.fit_transform(document.get('pdf_path'))  # Read the PDF file\n",
    "\n",
    "    # Process the document through each pipeline step\n",
    "    result = pipeline.named_steps['text_formatter'].transform(result)\n",
    "    #result = pipeline.named_steps['bullet_point_transformer'].transform(result)\n",
    "    result = pipeline.named_steps['sentence_chunker'].transform(result)\n",
    "    result = pipeline.named_steps['question_answer_generator'].transform(result)  # Generate questions and answers\n",
    "\n",
    "    # Generate embeddings and add them to the result\n",
    "    embeddings = pipeline.named_steps['embedding_generator'].transform(result, document)\n",
    "\n",
    "    # Save the embeddings and document data to ChromaDB\n",
    "    pipeline.named_steps['chromadb_saver'].transform(embeddings, [document] * len(embeddings))\n",
    "\n",
    "    # Process each chunk and add the data to the list\n",
    "    all_chunk_data = []\n",
    "    all_QandA =[]\n",
    "    for chunk in result:\n",
    "        chunk_data = {\n",
    "            \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "            \"chunk_char_count\": chunk[\"chunk_char_count\"],\n",
    "            \"chunk_word_count\": chunk[\"chunk_word_count\"],\n",
    "            \"chunk_token_count\": chunk[\"chunk_token_count\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"summary_text\": chunk[\"summary_text\"],\n",
    "            \"summary_char_count\": chunk[\"summary_char_count\"],\n",
    "            \"summary_word_count\": chunk[\"summary_word_count\"],\n",
    "            \"para_id\" : chunk[\"para_id\"],               \n",
    "        }\n",
    "        for index, question in enumerate(chunk[\"generated_questions\"], 0):\n",
    "            qa_data  = {\n",
    "               \n",
    "               \"page_number\": chunk[\"page_number\"],\n",
    "               \"para_id\" : chunk[\"para_id\"],   \n",
    "               \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "               \"question\" : question,\n",
    "               \"answer\" :  chunk[\"generated_answers\"][index]\n",
    "            }\n",
    "            all_QandA.append(qa_data)\n",
    "  \n",
    "        all_chunk_data.append(chunk_data)\n",
    "\n",
    "    # Convert the list of chunks into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_chunk_data)\n",
    "    df_qa = pd.DataFrame(all_QandA)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"manuals/\" + document_id + \".csv\", index=False)\n",
    "    df_qa.to_csv(\"manuals/\" + document_id + \"_QA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline Execution: Processing Multiple Documents\r\n",
    "\r\n",
    "This cell runs the document processing pipeline on a list of input documents. It demonstrates how to apply the pipeline to multiple documents in sequence.\r\n",
    "\r\n",
    "1. **Input Documents**:\r\n",
    "   - A list of dictionaries is defined, where each dictionary contains metadata for a document:\r\n",
    "     - `make`, `model`, `year`, and `style`: These are attributes of the document (e.g., the make and model of a vehicle or product).\r\n",
    "     - `pdf_path`: The file path to the PDF document that will be processed.\r\n",
    "   \r\n",
    "   Two example documents are provided:\r\n",
    "   - \"Fraggles X500 2024 FMS\"\r\n",
    "   - \"Fraggles X700 2022 HCM\"\r\n",
    "\r\n",
    "2. **Processing Each Document**:\r\n",
    "   - The code loops through each document in the `input_documents` list.\r\n",
    "   - For each document, the `process_document()` function is called, which processes the document using the predefined pipeline (as described in the previous markdown explanation).\r\n",
    "   - This includes reading the PDF, extracting and formatting text, chunking the text, generating question-answer pairs, creating embeddings, and saving results to ChromaDB and CSV files.\r\n",
    "\r\n",
    "By using this loop, you can easily process multiple documents in batch, allowing for scalable processing and storage of information for various documents in the collection.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Process each document\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m input_documents:\n\u001b[0;32m---> 20\u001b[0m     process_document(doc)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mprocess_document\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_document\u001b[39m(document):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Create the pipeline\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_reader\u001b[39m\u001b[38;5;124m'\u001b[39m, PDFReader),  \u001b[38;5;66;03m# Step 1: Read PDF (pass the class, not an instance)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_formatter\u001b[39m\u001b[38;5;124m'\u001b[39m, TextFormatter()),  \u001b[38;5;66;03m# Step 2: Format text\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m#('bullet_point_transformer', BulletPointTransformer()),  # Step 3: Transform bullet points\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_chunker\u001b[39m\u001b[38;5;124m'\u001b[39m, SentenceChunkerWithSummarization()),  \u001b[38;5;66;03m# Step 4: Chunk sentences\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_answer_generator\u001b[39m\u001b[38;5;124m'\u001b[39m, QuestionAnswerGenerator()),  \u001b[38;5;66;03m# Step 5: Generate QA pairs (call the class)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_generator\u001b[39m\u001b[38;5;124m'\u001b[39m, EmbeddingGenerator()),  \u001b[38;5;66;03m# Step 6: Generate embeddings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchromadb_saver\u001b[39m\u001b[38;5;124m'\u001b[39m, ChromaDBSaver())  \u001b[38;5;66;03m# Step 7: Save to ChromaDB\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     ])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Create a document ID based on attributes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     document_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/capstone/pipeline/embedding_proccessor.py:6\u001b[0m, in \u001b[0;36mEmbeddingGenerator.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:347\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_documents = [\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X500\",\n",
    "        \"year\": \"2024\",\n",
    "        \"style\": \"FMS\",\n",
    "        \"pdf_path\": \"manuals/FragglesX500FMS-2024-V4.pdf\"  \n",
    "    },\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X700\",\n",
    "        \"year\": \"2022\",\n",
    "        \"style\": \"HCM\",\n",
    "        \"pdf_path\": \"manuals/FragglesX700HCM-2022-V2.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each document\n",
    "for doc in input_documents:\n",
    "    process_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage: Searching with ChromaDB\n",
    "\n",
    "This cell demonstrates how to use the `ChromaDBSearcher` class to search for relevant context within a document using a query.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - A `ChromaDBSearcher` object (`searcher`) is instantiated. This object will interact with a ChromaDB collection to retrieve relevant document chunks.\n",
    "   \n",
    "2. **Setting the Document Source**:\n",
    "   - The variable `document_source` is set to `\"Fraggles_X500_2024_FMS\"`, which is the document ID you wish to search within.\n",
    "   - You can replace this with any other document ID (e.g., `\"Ford_Mustang_2023_MACH-E\"`) depending on the document you are interested in.\n",
    "\n",
    "3. **Defining the Query**:\n",
    "   - The `query` variable contains the text string `\"how to use parking breakes?\"`, which will be used to search for relevant answers within the specified document.\n",
    "   - This is the search term or question for which you want to find relevant content from the document.\n",
    "\n",
    "4. **Performing the Search**:\n",
    "   - The `search_by_id()` method of `ChromaDBSearcher` is called with the `document_source` and `query` as arguments.\n",
    "   - This method will return the top results (up to 10 by default) based on the relevance of the query and the document chunks stored in ChromaDB.\n",
    "\n",
    "This demonstrates how you can use ChromaDB to search for specific information in a document based on a query. It retrieves relevant text chunks that may provide an answer or context to the question posed in the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "searcher = ChromaDBSearcher()\n",
    "document_source = \"Fraggles_X500_2024_FMS\"  # Replace with the actual document ID you want to search for\n",
    "#document_source = \"Ford_Mustang_2023_MACH-E\"\n",
    "query = \"how to use parking breakes?\"  # Replace with the query you want to search for\n",
    "\n",
    "searcher.search_by_id(document_source, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 20:57:32 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             37W /  250W |   10449MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-16GB           On  |   00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             38W /  250W |     399MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    828814      C   ...n3.11-anaconda/2024.02-1/bin/python      10446MiB |\n",
      "|    1   N/A  N/A    895115      C   ...n3.11-anaconda/2024.02-1/bin/python        396MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
