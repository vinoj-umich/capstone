{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from tqdm.auto import tqdm  # For progress bars\n",
    "import re  # For regex operations\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer  # For embeddings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import logging\n",
    "import spacy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "def log_output(string):\n",
    "    #logger.info(string)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Text Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFormatter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        formatted_texts = []\n",
    "        \n",
    "        for pages_and_text in X:\n",
    "            # Replace newlines with spaces and strip leading/trailing spaces\n",
    "            formatted_text = pages_and_text['text'].replace(\"\\n\", \" \").strip()\n",
    "            formatted_page_text = {\"page_number\": pages_and_text['page_number'], \"formatted_text\": formatted_text}\n",
    "            formatted_texts.append(formatted_page_text)\n",
    "\n",
    "        log_output(\"Formatted texts successfully.\")\n",
    "        return formatted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Open and Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFReader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        \"\"\"\n",
    "        Reads a PDF file and extracts text from each page.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing the page number and its text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(self.pdf_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to open PDF file: {self.pdf_path}. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "        pages_and_texts = []\n",
    "        for page_number in tqdm(range(len(doc)), desc=\"Reading PDF pages\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text()\n",
    "            pages_and_texts.append({\"page_number\": page_number, \"text\": text})\n",
    "\n",
    "        logger.info(f\"Successfully read {len(pages_and_texts)} pages from {self.pdf_path}\")\n",
    "        return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Transformer to detect and convert bullet points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BulletPointTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to detect and convert bullet points into a structured format.\n",
    "    This ensures only successive bullet points are combined into one sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the BulletPointTransformer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method does nothing as the transformer doesn't require fitting.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data by identifying and formatting bullet points.\n",
    "        \n",
    "        :param X: List of documents or paragraphs to process\n",
    "        :return: List of documents with bullet points properly formatted\n",
    "        \"\"\"\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        transformed_data = []\n",
    "\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    # Apply bullet point transformation\n",
    "                    transformed_text = self._transform_bullet_points(text)\n",
    "                    item['formatted_text'] = transformed_text\n",
    "                    transformed_data.append(item)\n",
    "                else:\n",
    "                    logger.warning(f\"Missing 'formatted_text' key in item: {item}\")\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "        \n",
    "        return transformed_data\n",
    "\n",
    "    def _transform_bullet_points(self, text):\n",
    "        \"\"\"\n",
    "        Detect and combine only successive bullet points into a single sentence.\n",
    "        Bullet points can be identified by common characters such as *, -, or •.\n",
    "        \n",
    "        :param text: The input text to process.\n",
    "        :return: Text with only successive bullet points combined into a single sentence.\n",
    "        \"\"\"\n",
    "        # Regular expression to match bullet points (handles *, -, or •)\n",
    "        bullet_point_pattern = r'([*\\-•])\\s?(.*?)(?=\\n|\\r|\\Z|\\s*$)'  # Match bullet points\n",
    "\n",
    "        # Match all bullet points\n",
    "        bullet_points = re.findall(bullet_point_pattern, text)\n",
    "\n",
    "        # If there are bullet points, combine only successive ones into a single sentence\n",
    "        if bullet_points:\n",
    "            # Iterate through the bullet points and combine only successive ones\n",
    "            combined_bullet_points = []\n",
    "            last_bullet = None\n",
    "            for bp in bullet_points:\n",
    "                if last_bullet is None:  # First bullet point\n",
    "                    combined_bullet_points.append(bp[1].strip())\n",
    "                else:  # Add only if successive bullet points\n",
    "                    combined_bullet_points.append(bp[1].strip())\n",
    "                last_bullet = bp\n",
    "            \n",
    "            # Join all successive bullet points into a single sentence\n",
    "            combined_bullet_points_sentence = \" \".join(combined_bullet_points) + \".\"\n",
    "            # Replace the bullet points section with the combined sentence\n",
    "            text = re.sub(bullet_point_pattern, \"\", text)  # Remove old bullet points\n",
    "            text = f\"{combined_bullet_points_sentence} {text}\"  # Add combined bullet points as a sentence\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceChunkerWithSummarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import spacy\n",
    "import logging\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceChunkerWithSummarization(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sentences_per_chunk=10, max_summary_length=500, num_beams=4):\n",
    "        \"\"\"\n",
    "        Initialize the SentenceChunkerWithSummarization.\n",
    "        \n",
    "        :param max_sentences_per_chunk: The maximum number of sentences per chunk.\n",
    "        :param max_summary_length: Maximum length of the generated summary.\n",
    "        :param num_beams: Number of beams for beam search during summary generation.\n",
    "        \"\"\"\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "        self.max_summary_length = max_summary_length\n",
    "        self.num_beams = num_beams\n",
    "        \n",
    "        # Load the SpaCy model and add the sentencizer\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "        \n",
    "        # Load the T5 model and tokenizer\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method does nothing as the model doesn't require fitting.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def generate_summary(self, text):\n",
    "        \"\"\"\n",
    "        Generate a summary for a given text using the T5 model.\n",
    "        \n",
    "        :param text: The input text to summarize\n",
    "        :return: The summarized text\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            logger.warning(\"Received invalid text input.\")\n",
    "            return \"Invalid input: Empty or non-string text\"\n",
    "\n",
    "        # Tokenize the input with the T5 summarization prompt\n",
    "        input_tokens = self.tokenizer.encode(\"summarize: \" + text, return_tensors='pt')\n",
    "\n",
    "        # Generate the summary using the model\n",
    "        output = self.model.generate(input_tokens, max_length=self.max_summary_length, num_beams=self.num_beams, early_stopping=True)\n",
    "\n",
    "        # Decode the summary\n",
    "        summary = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def generate_unique_id(self, sentence_chunk):\n",
    "        \"\"\"\n",
    "        Generate a unique ID from a sentence chunk using SHA-256 hash.\n",
    "\n",
    "        :param sentence_chunk: The input sentence to generate the ID from.\n",
    "        :return: A unique ID (SHA-256 hash) as a hexadecimal string.\n",
    "        \"\"\"\n",
    "        # Step 1: Preprocess the sentence (optional, you could strip, lowercase, etc.)\n",
    "        processed_chunk = sentence_chunk.strip().lower()\n",
    "\n",
    "        # Step 2: Create the SHA-256 hash of the sentence\n",
    "        unique_id = hashlib.sha256(processed_chunk.encode()).hexdigest()\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data by chunking sentences and summarizing each chunk.\n",
    "        \n",
    "        :param X: List of documents or paragraphs to process\n",
    "        :return: List of dictionaries with sentence chunks and their summaries\n",
    "        \"\"\"\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        pages_and_chunks = []\n",
    "        sentences = []\n",
    "        pages = []\n",
    "\n",
    "        # Extract sentences and page numbers from the input\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item and 'page_number' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    page_number = item['page_number']\n",
    "                    if text:  # Check if text is not empty\n",
    "                        doc = self.nlp(text)  # Process text with SpaCy\n",
    "                        for sent in doc.sents:\n",
    "                            sentences.append(sent.text.strip())\n",
    "                            pages.append(page_number)\n",
    "                        logger.info(f\"Extracted sentences from page: {page_number}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty sentence found in item: {item}\")\n",
    "                else:\n",
    "                    logger.error(f\"Missing keys in item: {item}\")\n",
    "            elif isinstance(item, tuple) and len(item) == 2:\n",
    "                text = item[0].strip()\n",
    "                page_number = item[1]\n",
    "                doc = self.nlp(text)  # Process text with SpaCy\n",
    "                for sent in doc.sents:\n",
    "                    sentences.append(sent.text.strip())\n",
    "                    pages.append(page_number)\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "\n",
    "        # Organize sentences by page\n",
    "        sentences_by_page = {}\n",
    "        for sentence, page in zip(sentences, pages):\n",
    "            sentences_by_page.setdefault(page, []).append(sentence)\n",
    "\n",
    "        # Chunk sentences into fixed-size chunks and generate summaries\n",
    "        for page, sentences in sentences_by_page.items():\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            for i in range(0, len(sentences), self.max_sentences_per_chunk):\n",
    "                chunk_sentences = sentences[i:i + self.max_sentences_per_chunk]\n",
    "                chunk_text = \" \".join(chunk_sentences)\n",
    "                \n",
    "                # Generate the summary for the chunk of sentences\n",
    "                summary = self.generate_summary(chunk_text)\n",
    "\n",
    "                # Generate additional information\n",
    "                chunk_char_count = sum(len(s) for s in chunk_sentences)\n",
    "                chunk_word_count = sum(len(s.split()) for s in chunk_sentences)\n",
    "                chunk_token_count = sum(len(s) // 4 for s in chunk_sentences)\n",
    "                summary_char_count = len(summary)\n",
    "                summary_word_count = len(summary.split())\n",
    "\n",
    "                # Create a dictionary with both chunk data and summary data\n",
    "                chunk_dict = {\n",
    "                    \"sentence_chunk\": chunk_text,\n",
    "                    \"chunk_char_count\": chunk_char_count,\n",
    "                    \"chunk_word_count\": chunk_word_count,\n",
    "                    \"chunk_token_count\": chunk_token_count,\n",
    "                    \"page_number\": page,  # Include the page number\n",
    "                    \"summary_text\": summary,\n",
    "                    \"summary_char_count\": summary_char_count,\n",
    "                    \"summary_word_count\": summary_word_count,                    \n",
    "                    \"para_id\" : self.generate_unique_id(chunk_text)\n",
    "                }\n",
    "\n",
    "                # Only include chunks with more than 30 tokens\n",
    "                if chunk_token_count > 30:\n",
    "                    #logger.info(f\"Generated chunk and summary: {chunk_dict}\")\n",
    "                    pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "        #logger.info(f\"Processed {len(pages_and_chunks)} semantic chunks with summaries.\")\n",
    "        return pages_and_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuestionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class QuestionAnswerGenerator:\n",
    "    def __init__(self):\n",
    "        # Load Doc2Query model for question generation\n",
    "        self.model_name = 'doc2query/all-with_prefix-t5-base-v1'\n",
    "        self.qgen_tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.qgen_model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Load BERT or similar model for Question Answering (QA)\n",
    "        self.qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    def generate_questions(self, chunk, num_questions=5):\n",
    "        \"\"\"\n",
    "        Generate questions from a chunk of text using the Doc2Query model.\n",
    "\n",
    "        :param chunk: The input chunk of text to generate questions for.\n",
    "        :param num_questions: The number of questions to generate (default is 5).\n",
    "        :return: A list of generated questions.\n",
    "        \"\"\"\n",
    "        # Prepare the chunk for Doc2Query\n",
    "        input_text = f\"generate questions: {chunk}\"\n",
    "        inputs = self.qgen_tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "        # Check if we are using greedy decoding or beam search\n",
    "        if num_questions == 1:\n",
    "            # Use greedy decoding for one question\n",
    "            outputs = self.qgen_model.generate(\n",
    "                **inputs, \n",
    "                max_length=50, \n",
    "                num_return_sequences=1,  # Only generate one question\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        else:\n",
    "            # Use beam search for multiple questions\n",
    "            outputs = self.qgen_model.generate(\n",
    "                **inputs, \n",
    "                max_length=50, \n",
    "                num_return_sequences=num_questions,  # Generate multiple questions\n",
    "                num_beams=num_questions,  # Use beam search\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "\n",
    "        # Decode the generated questions\n",
    "        questions = [self.qgen_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        return questions\n",
    "\n",
    "    def generate_answers(self, chunk, questions):\n",
    "        \"\"\"\n",
    "        Generate answers for a list of questions given a chunk of text.\n",
    "\n",
    "        :param chunk: The input chunk of text for answering the questions.\n",
    "        :param questions: A list of questions to answer.\n",
    "        :return: A list of answers corresponding to the input questions.\n",
    "        \"\"\"\n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            # Encode the question and the context (chunk) for QA model\n",
    "            inputs = self.qa_tokenizer.encode_plus(question, chunk, return_tensors='pt')\n",
    "\n",
    "            # Get the start and end positions of the answer\n",
    "            outputs = self.qa_model(**inputs)\n",
    "\n",
    "            # If the model outputs a tuple (start_scores, end_scores)\n",
    "            if isinstance(outputs, tuple):\n",
    "                answer_start_scores, answer_end_scores = outputs\n",
    "            else:\n",
    "                # If the model returns a dict, extract the start and end scores\n",
    "                answer_start_scores = outputs['start_logits']\n",
    "                answer_end_scores = outputs['end_logits']\n",
    "\n",
    "            # Get the most likely beginning and end of the answer\n",
    "            start_index = torch.argmax(answer_start_scores)\n",
    "            end_index = torch.argmax(answer_end_scores)\n",
    "\n",
    "            # Decode the answer from the token indices\n",
    "            answer = self.qa_tokenizer.convert_tokens_to_string(\n",
    "                self.qa_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1])\n",
    "            )\n",
    "\n",
    "            answers.append(answer)\n",
    "        return answers\n",
    "\n",
    "    def transform(self, chunk_data):\n",
    "        \"\"\"\n",
    "        Transform the input chunk data by generating questions and answers.\n",
    "\n",
    "        :param chunk_data: A list of chunks, each containing a sentence chunk.\n",
    "        :return: A list of chunks with generated questions and answers added.\n",
    "        \"\"\"\n",
    "        all_chunk_qa = []\n",
    "        for chunk in chunk_data:\n",
    "            chunk_text = chunk['sentence_chunk']  # Get the text of the chunk\n",
    "            questions = self.generate_questions(chunk_text)\n",
    "            answers = self.generate_answers(chunk_text, questions)\n",
    "            chunk['generated_questions'] = questions\n",
    "            chunk['generated_answers'] = answers\n",
    "            all_chunk_qa.append(chunk)\n",
    "        return all_chunk_qa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 3: Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sentences_per_chunk=5):\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "        # Load the SpaCy English model and add the sentencizer\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes=None):\n",
    "        pages_and_chunks = []\n",
    "        logger.info(f\"Input data for transformation: {X}\")\n",
    "        logger.info(f\"Input data length: {len(X)}\")\n",
    "\n",
    "        if not X:\n",
    "            logger.warning(\"Input data is empty.\")\n",
    "            return []\n",
    "\n",
    "        sentences = []\n",
    "        pages = []\n",
    "\n",
    "        # Extract sentences and page numbers\n",
    "        for item in X:\n",
    "            if isinstance(item, dict):\n",
    "                if 'formatted_text' in item and 'page_number' in item:\n",
    "                    text = item['formatted_text'].strip()\n",
    "                    page_number = item['page_number']\n",
    "                    if text:  # Check if text is not empty\n",
    "                        doc = self.nlp(text)  # Process text with SpaCy\n",
    "                        for sent in doc.sents:\n",
    "                            sentences.append(sent.text.strip())\n",
    "                            pages.append(page_number)\n",
    "                        logger.info(f\"Extracted sentences from page: {page_number}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty sentence found in item: {item}\")\n",
    "                else:\n",
    "                    logger.error(f\"Missing keys in item: {item}\")\n",
    "            elif isinstance(item, tuple) and len(item) == 2:\n",
    "                text = item[0].strip()\n",
    "                page_number = item[1]\n",
    "                doc = self.nlp(text)  # Process text with SpaCy\n",
    "                for sent in doc.sents:\n",
    "                    sentences.append(sent.text.strip())\n",
    "                    pages.append(page_number)\n",
    "            else:\n",
    "                logger.error(f\"Unexpected item format: {item}\")\n",
    "\n",
    "        # Organize sentences by pages\n",
    "        sentences_by_page = {}\n",
    "        for sentence, page in zip(sentences, pages):\n",
    "            sentences_by_page.setdefault(page, []).append(sentence)\n",
    "\n",
    "        for page, sentences in sentences_by_page.items():\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            # Chunk sentences into fixed-size chunks\n",
    "            for i in range(0, len(sentences), self.max_sentences_per_chunk):\n",
    "                chunk_sentences = sentences[i:i + self.max_sentences_per_chunk]\n",
    "                chunk_token_count = sum(len(s) // 4 for s in chunk_sentences)\n",
    "                chunk_dict = {\n",
    "                    \"sentence_chunk\": \" \".join(chunk_sentences),\n",
    "                    \"chunk_char_count\": sum(len(s) for s in chunk_sentences),\n",
    "                    \"chunk_word_count\": sum(len(s.split()) for s in chunk_sentences),\n",
    "                    \"chunk_token_count\": sum(len(s) // 4 for s in chunk_sentences),  # Adjust if needed\n",
    "                    \"page_number\": page  # Include the page number\n",
    "                }\n",
    "                if chunk_token_count > 30:\n",
    "                    logger.info(f\"Generated chunk: {chunk_dict}\")\n",
    "                    pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "        logger.info(f\"Processed {len(pages_and_chunks)} semantic chunks.\")\n",
    "        return pages_and_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes):\n",
    "        sentences = [chunk[\"sentence_chunk\"] for chunk in X]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        \n",
    "        for i, chunk in enumerate(X):\n",
    "            chunk[\"embedding\"] = embeddings[i]\n",
    "        \n",
    "        #log_output(\"Embedding Generator: \"+ len(X))  # Log output\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Save to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBSaver(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, chroma_db_dir=\"app/chroma_db_dir\"):  # Ensure this points to your local ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "        self.collection = self.client.get_or_create_collection(\"pdf_chunks\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, document_attributes):\n",
    "        i = 0 \n",
    "        for chunk, doc_attr in zip(X, document_attributes):\n",
    "\n",
    "            document_id = f\"{doc_attr['make']}_{doc_attr['model']}_{doc_attr['year']}_{doc_attr['style']}\"\n",
    "            \n",
    "            # Log the chunk being added\n",
    "            text =  chunk[\"sentence_chunk\"]\n",
    "            chunk_char_count = chunk[\"chunk_char_count\"]\n",
    "            chunk_word_count = chunk[\"chunk_word_count\"]\n",
    "            if chunk[\"sentence_chunk\"].strip():  # Ensure it's not empty\n",
    "                print(f\"Adding document ID: {document_id}, Content: '{chunk['sentence_chunk']}'\")\n",
    "                \n",
    "                self.collection.add(\n",
    "                    documents=[text],\n",
    "                    embeddings=[chunk[\"embedding\"].tolist()],\n",
    "                    metadatas=[{\"source\": document_id}],\n",
    "                    ids = [f\"{document_id}_{chunk['page_number']}_{chunk_word_count}_{chunk_char_count}\"]\n",
    "\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping empty document for ID: {document_id}\")\n",
    "            i=i+1\n",
    "\n",
    "        log_output(\"ChromaDB Saver , Data saved to ChromaDB\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('pdf_reader', PDFReader),  # Step 1: Read PDF (pass the class, not an instance)\n",
    "        ('text_formatter', TextFormatter()),  # Step 2: Format text\n",
    "        ('bullet_point_transformer', BulletPointTransformer()),  # Step 3: Transform bullet points\n",
    "        ('sentence_chunker', SentenceChunkerWithSummarization()),  # Step 4: Chunk sentences\n",
    "        ('question_answer_generator', QuestionAnswerGenerator()),  # Step 5: Generate QA pairs (call the class)\n",
    "        ('embedding_generator', EmbeddingGenerator()),  # Step 6: Generate embeddings\n",
    "        ('chromadb_saver', ChromaDBSaver())  # Step 7: Save to ChromaDB\n",
    "    ])\n",
    "\n",
    "    # Create a document ID based on attributes\n",
    "    document_id = f\"{document['make']}_{document['model']}_{document['year']}_{document['style']}\"\n",
    "\n",
    "    # Instantiate PDFReader manually, as it requires the file path\n",
    "    pdf_reader = PDFReader(document.get('pdf_path'))\n",
    "    result = pdf_reader.fit_transform(document.get('pdf_path'))  # Read the PDF file\n",
    "\n",
    "    # Process the document through each pipeline step\n",
    "    result = pipeline.named_steps['text_formatter'].transform(result)\n",
    "    result = pipeline.named_steps['bullet_point_transformer'].transform(result)\n",
    "    result = pipeline.named_steps['sentence_chunker'].transform(result)\n",
    "    result = pipeline.named_steps['question_answer_generator'].transform(result)  # Generate questions and answers\n",
    "\n",
    "    # Generate embeddings and add them to the result\n",
    "    embeddings = pipeline.named_steps['embedding_generator'].transform(result, document)\n",
    "\n",
    "    # Save the embeddings and document data to ChromaDB\n",
    "    pipeline.named_steps['chromadb_saver'].transform(embeddings, [document] * len(embeddings))\n",
    "\n",
    "    # Process each chunk and add the data to the list\n",
    "    all_chunk_data = []\n",
    "    all_QandA =[]\n",
    "    for chunk in result:\n",
    "        chunk_data = {\n",
    "            \"sentence_chunk\": chunk[\"sentence_chunk\"],\n",
    "            \"chunk_char_count\": chunk[\"chunk_char_count\"],\n",
    "            \"chunk_word_count\": chunk[\"chunk_word_count\"],\n",
    "            \"chunk_token_count\": chunk[\"chunk_token_count\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"summary_text\": chunk[\"summary_text\"],\n",
    "            \"summary_char_count\": chunk[\"summary_char_count\"],\n",
    "            \"summary_word_count\": chunk[\"summary_word_count\"],\n",
    "            \"para_id\" : chunk[\"para_id\"],               \n",
    "        }\n",
    "        for index, question in enumerate(chunk[\"generated_questions\"], 0):\n",
    "            qa_data  = {\n",
    "               \"para_id\" : chunk[\"para_id\"],   \n",
    "               \"question\" : question,\n",
    "               \"answer\" :  chunk[\"generated_answers\"][index]\n",
    "            }\n",
    "            all_QandA.append(qa_data)\n",
    "  \n",
    "        all_chunk_data.append(chunk_data)\n",
    "\n",
    "    # Convert the list of chunks into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_chunk_data)\n",
    "    df_qa = pd.DataFrame(all_QandA)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"/app/manuals/\" + document_id + \".csv\", index=False)\n",
    "    df_qa.to_csv(\"/app/manuals/\" + document_id + \"_QA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ab8a089784498fab4ea5e9b263c226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully read 97 pages from /app/manuals/FragglesX500FMS-V3.pdf\n",
      "INFO:__main__:Extracted sentences from page: 0\n",
      "INFO:__main__:Extracted sentences from page: 1\n",
      "INFO:__main__:Extracted sentences from page: 2\n",
      "INFO:__main__:Extracted sentences from page: 3\n",
      "INFO:__main__:Extracted sentences from page: 4\n",
      "INFO:__main__:Extracted sentences from page: 5\n",
      "INFO:__main__:Extracted sentences from page: 6\n",
      "INFO:__main__:Extracted sentences from page: 7\n",
      "INFO:__main__:Extracted sentences from page: 8\n",
      "INFO:__main__:Extracted sentences from page: 9\n",
      "INFO:__main__:Extracted sentences from page: 10\n",
      "INFO:__main__:Extracted sentences from page: 11\n",
      "INFO:__main__:Extracted sentences from page: 12\n",
      "INFO:__main__:Extracted sentences from page: 13\n",
      "INFO:__main__:Extracted sentences from page: 14\n",
      "INFO:__main__:Extracted sentences from page: 15\n",
      "INFO:__main__:Extracted sentences from page: 16\n",
      "INFO:__main__:Extracted sentences from page: 17\n",
      "INFO:__main__:Extracted sentences from page: 18\n",
      "INFO:__main__:Extracted sentences from page: 19\n",
      "INFO:__main__:Extracted sentences from page: 20\n",
      "INFO:__main__:Extracted sentences from page: 21\n",
      "INFO:__main__:Extracted sentences from page: 22\n",
      "INFO:__main__:Extracted sentences from page: 23\n",
      "INFO:__main__:Extracted sentences from page: 24\n",
      "INFO:__main__:Extracted sentences from page: 25\n",
      "INFO:__main__:Extracted sentences from page: 26\n",
      "INFO:__main__:Extracted sentences from page: 27\n",
      "INFO:__main__:Extracted sentences from page: 28\n",
      "INFO:__main__:Extracted sentences from page: 29\n",
      "INFO:__main__:Extracted sentences from page: 30\n",
      "INFO:__main__:Extracted sentences from page: 31\n",
      "INFO:__main__:Extracted sentences from page: 32\n",
      "INFO:__main__:Extracted sentences from page: 33\n",
      "INFO:__main__:Extracted sentences from page: 34\n",
      "INFO:__main__:Extracted sentences from page: 35\n",
      "INFO:__main__:Extracted sentences from page: 36\n",
      "INFO:__main__:Extracted sentences from page: 37\n",
      "INFO:__main__:Extracted sentences from page: 38\n",
      "INFO:__main__:Extracted sentences from page: 39\n",
      "INFO:__main__:Extracted sentences from page: 40\n",
      "INFO:__main__:Extracted sentences from page: 41\n",
      "INFO:__main__:Extracted sentences from page: 42\n",
      "INFO:__main__:Extracted sentences from page: 43\n",
      "INFO:__main__:Extracted sentences from page: 44\n",
      "INFO:__main__:Extracted sentences from page: 45\n",
      "INFO:__main__:Extracted sentences from page: 46\n",
      "INFO:__main__:Extracted sentences from page: 47\n",
      "INFO:__main__:Extracted sentences from page: 48\n",
      "INFO:__main__:Extracted sentences from page: 49\n",
      "INFO:__main__:Extracted sentences from page: 50\n",
      "INFO:__main__:Extracted sentences from page: 51\n",
      "INFO:__main__:Extracted sentences from page: 52\n",
      "INFO:__main__:Extracted sentences from page: 53\n",
      "INFO:__main__:Extracted sentences from page: 54\n",
      "INFO:__main__:Extracted sentences from page: 55\n",
      "INFO:__main__:Extracted sentences from page: 56\n",
      "INFO:__main__:Extracted sentences from page: 57\n",
      "INFO:__main__:Extracted sentences from page: 58\n",
      "INFO:__main__:Extracted sentences from page: 59\n",
      "INFO:__main__:Extracted sentences from page: 60\n",
      "INFO:__main__:Extracted sentences from page: 61\n",
      "INFO:__main__:Extracted sentences from page: 62\n",
      "INFO:__main__:Extracted sentences from page: 63\n",
      "INFO:__main__:Extracted sentences from page: 64\n",
      "INFO:__main__:Extracted sentences from page: 65\n",
      "INFO:__main__:Extracted sentences from page: 66\n",
      "INFO:__main__:Extracted sentences from page: 67\n",
      "INFO:__main__:Extracted sentences from page: 68\n",
      "INFO:__main__:Extracted sentences from page: 69\n",
      "INFO:__main__:Extracted sentences from page: 70\n",
      "INFO:__main__:Extracted sentences from page: 71\n",
      "INFO:__main__:Extracted sentences from page: 72\n",
      "INFO:__main__:Extracted sentences from page: 73\n",
      "INFO:__main__:Extracted sentences from page: 74\n",
      "INFO:__main__:Extracted sentences from page: 75\n",
      "INFO:__main__:Extracted sentences from page: 76\n",
      "INFO:__main__:Extracted sentences from page: 77\n",
      "INFO:__main__:Extracted sentences from page: 78\n",
      "INFO:__main__:Extracted sentences from page: 79\n",
      "INFO:__main__:Extracted sentences from page: 80\n",
      "INFO:__main__:Extracted sentences from page: 81\n",
      "INFO:__main__:Extracted sentences from page: 82\n",
      "INFO:__main__:Extracted sentences from page: 83\n",
      "INFO:__main__:Extracted sentences from page: 84\n",
      "INFO:__main__:Extracted sentences from page: 85\n",
      "INFO:__main__:Extracted sentences from page: 86\n",
      "INFO:__main__:Extracted sentences from page: 87\n",
      "INFO:__main__:Extracted sentences from page: 88\n",
      "INFO:__main__:Extracted sentences from page: 89\n",
      "INFO:__main__:Extracted sentences from page: 90\n",
      "INFO:__main__:Extracted sentences from page: 91\n",
      "INFO:__main__:Extracted sentences from page: 92\n",
      "INFO:__main__:Extracted sentences from page: 93\n",
      "INFO:__main__:Extracted sentences from page: 94\n",
      "INFO:__main__:Extracted sentences from page: 95\n",
      "INFO:__main__:Extracted sentences from page: 96\n"
     ]
    }
   ],
   "source": [
    "# Main Pipeline Execution\n",
    "input_documents = [\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X500\",\n",
    "        \"year\": \"2027\",\n",
    "        \"style\": \"FMS\",\n",
    "        \"pdf_path\": \"/app/manuals/FragglesX500FMS-V3.pdf\"  \n",
    "    },\n",
    "    {\n",
    "        \"make\": \"Fraggles\",\n",
    "        \"model\": \"X700\",\n",
    "        \"year\": \"2026\",\n",
    "        \"style\": \"CRV\",\n",
    "        \"pdf_path\": \"/app/manuals/FragglesX700CRV-v1.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each document\n",
    "for doc in input_documents:\n",
    "    process_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBSearcher:\n",
    "    def __init__(self, chroma_db_dir=\"app/chroma_db_dir\", model_name=\"all-mpnet-base-v2\"):\n",
    "        self.client = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "        self.collection = self.client.get_collection(\"pdf_chunks\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def search_by_id(self, document_id, query):\n",
    "        try:\n",
    "            query_embedding = self.model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
    "            results = self.collection.query(\n",
    "                query_embedding.tolist(),\n",
    "                where={\"source\": document_source},\n",
    "                n_results=5\n",
    "            )\n",
    "            \n",
    "            if results and results['documents']:\n",
    "                document_content = results['documents'][0]  # Remove extra spaces\n",
    "                if document_content:\n",
    "                    return results['documents']\n",
    "                else:\n",
    "                    print(\"Document content is empty.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"No documents found.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search by ID: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "searcher = ChromaDBSearcher()\n",
    "document_source = \"Chrysler_Pacifica_2017_Limited\"  # Replace with the actual document ID you want to search for\n",
    "#document_source = \"Ford_Mustang_2023_MACH-E\"\n",
    "query = \"how to use parking breaks?\"  # Replace with the query you want to search for\n",
    "\n",
    "searcher.search_by_id(document_source, query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
