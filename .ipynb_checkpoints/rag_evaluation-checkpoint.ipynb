{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517b0fb8-2c58-4bd6-a20b-ce600971068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard Libraries\n",
    "import warnings\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "# 2. Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 3. Natural Language Processing (NLP) Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "# 4. Scoring and Evaluation Metrics\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# 5. Transformers Library (for Model Loading and Tokenization)\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer  # For embeddings\n",
    "\n",
    "\n",
    "# 6. ChromaDBSearcher & ChatModel\n",
    "from common.chroma_db import ChromaDBSearcher\n",
    "from common.chat_model import ModelQA\n",
    "\n",
    "# 7. Logging Configuration\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "# 8. Warning Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9762bea-fac1-4955-b711-90342cb51898",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\r\n",
    "\r\n",
    "This cell loads and preprocesses a manually curated CSV file containing questions and answers. The steps ensure the data is cleaned and formatted correctly for further processing.\r\n",
    "\r\n",
    "1. **Setting up Paths**:\r\n",
    "   - The `ProjectRoot` variable is set to an empty string initially, but it can be configured to point to the root directory of your project.\r\n",
    "   - The `DatasetRoot` variable concatenates the `ProjectRoot` with the folder `\"manuals/\"`, where the dataset is stored.\r\n",
    "\r\n",
    "2. **Loading the CSV File**:\r\n",
    "   - The file path to the curated CSV file is constructed using `DatasetRoot` and the file name `Fraggles_X700_2022_HCM_QA_Curated.csv`.\r\n",
    "   - The `pandas` library (`pd.read_csv()`) is used to load the CSV file into a DataFrame (`df`). The file is read using the `latin1` encoding to handle any special characters properly.\r\n",
    "\r\n",
    "3. **Data Cleaning**:\r\n",
    "   - **Drop missing values**: Rows where either the `question` or `answer` column is missing are removed with `dropna()`.\r\n",
    "   - **Remove duplicates**: Rows with duplicate `para_id` values are removed using `drop_duplicates()`, keeping only the first occurrence of each `para_id`.\r\n",
    "   - **Reset index**: After cleaning, the index is reset to ensure the DataFrame has a consistent numbering scheme, which is done twice (`reset_index(dleaned properly.\r\n",
    "\r\n",
    "This step prepares the data for further processing by ensuring that only relevant and clean rcords are included.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967f6a51-4cde-487f-90f0-82d66899ae50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>page_number</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>e6c45229dc996b6ba2ecb9ddb251f77c5ee3f2de93cc1d...</td>\n",
       "      <td>check that the rod is ï¬rmly placed to avoid ...</td>\n",
       "      <td>What should you do to prevent the hood from ac...</td>\n",
       "      <td>Before closing the hood, make sure the support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>69dedd5e3d5bbf7c8e9a12176d60789e110a65cd533d26...</td>\n",
       "      <td>If you feel any resistance, stop and  check fo...</td>\n",
       "      <td>What should you do if you feel resistance when...</td>\n",
       "      <td>f you feel any resistance, stop immediately an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>fc843f5ed9e92f0a42868ed7813e7c197902b5f2194f87...</td>\n",
       "      <td>Pull the hood release handle: The hood release...</td>\n",
       "      <td>Where is the hood release handle typically loc...</td>\n",
       "      <td>The hood release handle is usually located in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0c927bca6c80fe20d270cb43d62ea95099483181509007...</td>\n",
       "      <td>The support rod is typically located  on the u...</td>\n",
       "      <td>Where is the support rod typically located in ...</td>\n",
       "      <td>The support rod is typically located on the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>560865acae00a7a1c7b35d8567b972195079a68eddcf40...</td>\n",
       "      <td>Genuine FragglesX700HCM Motor Oil (for optimal...</td>\n",
       "      <td>What are the benefits of using Genuine Fraggle...</td>\n",
       "      <td>Genuine FragglesX700HCM Motor Oil is a premium...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  page_number  \\\n",
       "0        0      0            1   \n",
       "1        1      3            1   \n",
       "2        2      6            1   \n",
       "3        3      9            1   \n",
       "4        4     12            2   \n",
       "\n",
       "                                             para_id  \\\n",
       "0  e6c45229dc996b6ba2ecb9ddb251f77c5ee3f2de93cc1d...   \n",
       "1  69dedd5e3d5bbf7c8e9a12176d60789e110a65cd533d26...   \n",
       "2  fc843f5ed9e92f0a42868ed7813e7c197902b5f2194f87...   \n",
       "3  0c927bca6c80fe20d270cb43d62ea95099483181509007...   \n",
       "4  560865acae00a7a1c7b35d8567b972195079a68eddcf40...   \n",
       "\n",
       "                                      sentence_chunk  \\\n",
       "0  check that the rod is ï¬rmly placed to avoid ...   \n",
       "1  If you feel any resistance, stop and  check fo...   \n",
       "2  Pull the hood release handle: The hood release...   \n",
       "3  The support rod is typically located  on the u...   \n",
       "4  Genuine FragglesX700HCM Motor Oil (for optimal...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What should you do to prevent the hood from ac...   \n",
       "1  What should you do if you feel resistance when...   \n",
       "2  Where is the hood release handle typically loc...   \n",
       "3  Where is the support rod typically located in ...   \n",
       "4  What are the benefits of using Genuine Fraggle...   \n",
       "\n",
       "                                              answer  \n",
       "0  Before closing the hood, make sure the support...  \n",
       "1  f you feel any resistance, stop immediately an...  \n",
       "2  The hood release handle is usually located in ...  \n",
       "3  The support rod is typically located on the un...  \n",
       "4  Genuine FragglesX700HCM Motor Oil is a premium...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide project root path\n",
    "ProjectRoot = \"\"\n",
    "DatasetRoot = ProjectRoot + \"manuals/\"\n",
    "\n",
    "# Load the manually curated CSV file\n",
    "file_path = DatasetRoot + '/Fraggles_X700_2022_HCM_QA_Curated.csv'\n",
    "df = pd.read_csv(file_path,  encoding='latin1')\n",
    "\n",
    "#keep only one question per chunk\n",
    "df.dropna(subset=['question', 'answer'], inplace=True)\n",
    "df = df.drop_duplicates(subset=['para_id'], keep='first')\n",
    "df = df.reset_index(drop=False)  # Adds the index as a new column\n",
    "df =df.reset_index(drop=False)\n",
    "\n",
    "# Check the first few rows to understand the structure\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481aef9-8d38-4b77-9a59-c85f2527234e",
   "metadata": {},
   "source": [
    "\n",
    "### Overview of Evaluation Metrics:\r\n",
    "\r\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: Measures how many n-grams in the generated text match n-grams in the reference text. Commonly used in machine translation evaluation.\r\n",
    "  \r\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Measures the overlap of n-grams, word sequences, and word pairs between the reference and generated text. Used for summarization tasks.\r\n",
    "  \r\n",
    "- **BERTScore**: Leverages BERT embeddings for a more context-sensitive evaluation of text generation.\r\n",
    "  \r\n",
    "- **METEOR**: Takes into account synonymy, stemming, and word order in its evaluation of generated text.\r\n",
    "  \r\n",
    "- **Cosine Similarity**: A measure of similarity between two vectors, commonly used in evaluating semantic similarity between sentences.\r\n",
    " model is performing.\r\n",
    "cords are included.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf13f9b-ada0-4a24-88b0-6a6a929d6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(reference: str, candidate: str) -> float:\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "def evaluate_rouge(reference: str, candidate: str) -> dict:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "# Function to calculate BERTScore\n",
    "def evaluate_bertscore(reference, generated):\n",
    "    P, R, F1 = bert_score([generated], [reference], lang='en', verbose=True)\n",
    "    return P.item(), R.item(), F1.item()\n",
    "\n",
    "# Function to calculate METEOR score\n",
    "def evaluate_meteor(reference, generated):\n",
    "    # Tokenize the reference and generated answers\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    generated_tokens = word_tokenize(generated)\n",
    "    \n",
    "    # Calculate and return the METEOR score\n",
    "    return meteor_score.single_meteor_score(reference_tokens, generated_tokens)\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "# Function to calculate Cosine Similarity\n",
    "def calculate_cosine_similarity(reference, generated):\n",
    "    # Encode sentences to embeddings\n",
    "    reference_embedding = embedding_model.encode([reference])\n",
    "    generated_embedding = embedding_model.encode([generated])\n",
    "    \n",
    "    # Compute cosine similarity between the two embeddings\n",
    "    similarity = cosine_similarity(reference_embedding, generated_embedding)\n",
    "    return similarity[0][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22b678-9c4d-4a90-be3f-522ce62b13a4",
   "metadata": {},
   "source": [
    "# **Model Evaluation Overview**\n",
    "\n",
    "This process evaluates text generation models using multiple NLP metrics. The evaluation steps involve generating answers to questions and scoring the generated responses across various models.\n",
    "\n",
    "## **Model List**\n",
    "- **Llama-2**: `meta-llama/Llama-2-7b-chat-hf`\n",
    "- **gemma-2**: `google/gemma-2b-it`\n",
    "- **GPT2**: `openai-community/gpt2-medium`\n",
    "- **phi2**: `microsoft/phi-2`\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow**\n",
    "1. **Load Data**: Curated CSV with questions and answers.\n",
    "2. **Process**: For each chunk (grouped by `para_id`), evaluate answers generated by multiple models.\n",
    "3. **Metrics Evaluation**: Use BLEU, ROUGE, BERTScore, METEOR, and Cosine Similarity to score each generated answer.\n",
    "4. **Result Storage**: Collect all scores in a structured format (DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17258403-5261-4dec-87de-0918ce635d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vinoj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*weights.*initialized.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the embedding model and searcher (ChromaDB)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m searcher \u001b[38;5;241m=\u001b[39m ChromaDBSearcher()  \u001b[38;5;66;03m# Assuming ChromaDBSearcher is defined elsewhere\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Function to load model id\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure nltk's punkt tokenizer is downloaded\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weights.*initialized.*\")\n",
    "\n",
    "# Initialize the embedding model and searcher (ChromaDB)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "searcher = ChromaDBSearcher()  # Assuming ChromaDBSearcher is defined elsewhere\n",
    "\n",
    "# Function to load model id\n",
    "def get_model_id(model_name):\n",
    "    if model_name == \"Llama-2\":\n",
    "        return \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    elif model_name == \"gemma-2\":\n",
    "        return \"google/gemma-2b-it\"\n",
    "    elif model_name == 'GPT1':\n",
    "        return \"openai-community/openai-gpt\"\n",
    "    elif model_name == \"GPT2\":\n",
    "        return \"openai-community/gpt2-medium\"\n",
    "    elif model_name == \"phi2\":\n",
    "        return \"microsoft/phi-2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "# Create an empty list to store the evaluation results\n",
    "evaluation_results = []\n",
    "models = [\"Llama-2\", \"gemma-2\", \"GPT2\", \"phi2\"]\n",
    "model_cache  = {}\n",
    "\n",
    "# Loop through the chunks in the dataframe (assuming each chunk is already grouped)\n",
    "for chunk_index, chunk in enumerate(df.groupby('para_id')):  # Assuming 'para_id' is the identifier\n",
    "    chunk_data = chunk[1]  # Get the actual chunk\n",
    "\n",
    "    # Process each question-answer pair in the chunk\n",
    "    for index, row in chunk_data.iterrows():\n",
    "        question = row['question']\n",
    "        reference_answer = row['answer']\n",
    "        \n",
    "        # Loop through each model to generate answers\n",
    "        for model_name in models:\n",
    "            # Load the model and tokenizer dynamically for each model using ModelQA class\n",
    "            model_id = get_model_id(model_name)\n",
    "            model_qa = model_cache.get(model_id)\n",
    "            if model_qa is None: \n",
    "                 model_qa = ModelQA(model_id=model_id, searcher=searcher)\n",
    "                 model_cache[model_id] =  model_qa \n",
    "            # Generate answer using the `ask` function\n",
    "            generated_answer = model_qa.ask(document_source=\"document_source\", query=question)\n",
    "            \n",
    "            # Evaluate BLEU score\n",
    "            bleu_score = evaluate_bleu(reference_answer, generated_answer)\n",
    "            \n",
    "            # Evaluate ROUGE score\n",
    "            rouge_scores = evaluate_rouge(reference_answer, generated_answer)\n",
    "            \n",
    "            # Evaluate BERTScore\n",
    "            bert_p, bert_r, bert_f1 = evaluate_bertscore(reference_answer, generated_answer)\n",
    "            \n",
    "            # Evaluate METEOR score\n",
    "            meteor = evaluate_meteor(reference_answer, generated_answer)\n",
    "\n",
    "            # Calculate Cosine Similarity\n",
    "            cosine_similarity_score = calculate_cosine_similarity(reference_answer, generated_answer)\n",
    "\n",
    "            # Append the results for this chunk, question, and model\n",
    "            evaluation_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Chunk\": chunk_index,\n",
    "                \"Question\": question,\n",
    "                \"BLEU\": bleu_score,\n",
    "                \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "                \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "                \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "                \"BERTScore_Precision\": bert_p,\n",
    "                \"BERTScore_Recall\": bert_r,\n",
    "                \"BERTScore_F1\": bert_f1,\n",
    "                \"METEOR\": meteor,\n",
    "                \"Cosine_Similarity\": cosine_similarity_score\n",
    "            })\n",
    "\n",
    "# Convert evaluation results into a DataFrame\n",
    "eval_df = pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defbbf0-a745-4107-9395-2214378109a7",
   "metadata": {},
   "source": [
    "## **Plotting the Evaluation Metrics for BLEU, ROUGE-1, ROUGE-2, and ROUGE-L**\r\n",
    "\r\n",
    "This bar chart compares the performance of different models (e.g., Llama-2, GPT2, phi2) based on several evaluation metrics: BLEU, ROUGE-1, ROUGE-2, and ROUGE-L. Each model is evaluated on these metrics, and the results are displayed as grouped bars for easy comparison. \r\n",
    "\r\n",
    "- **X-Axis**: Different models.\r\n",
    "- **Y-Axis**: Evaluation score values (BLEU, ROUGE-1, ROUGE-2, ROUGE-L).\r\n",
    "- **Grouped Bars**: Each model has separate bars for each metric, color-coded for clarity.\r\n",
    "\r\n",
    "This visualization allows us to quickly assess how each model performs on various NLP tasks, aiding in model comparison and selection.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab9d83-49a8-4e80-9ca7-bbf995b563d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Reshaping the data into a long format\n",
    "melted_scores = eval_df.melt(id_vars=\"Model\", \n",
    "                                    value_vars=[\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
    "                                    var_name=\"Metric\", \n",
    "                                    value_name=\"Score\")\n",
    "\n",
    "# Set the plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the grouped bar chart\n",
    "plt.figure(figsize=(24, 8))\n",
    "ax = sns.barplot(x=\"Model\", y=\"Score\", hue=\"Metric\", data=melted_scores, palette=\"Set2\")\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Comparison of BLEU and ROUGE Scores for Different Models\", fontsize=16)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add annotations for each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=12, color='black',\n",
    "                xytext=(0, 8), textcoords='offset points')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend(title=\"Metric\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89f48b-28b0-485f-9323-eff85a9e5a8e",
   "metadata": {},
   "source": [
    "# Plotting the Evaluation Metrics for BERT, METEOR &  Cosine Similarity Scores\n",
    "\n",
    "\r\n",
    "This chart compares multiple models based on several evaluation metrics:\r\n",
    "\r\n",
    "- **BERTScore (Precision, Recall, F1)**\r\n",
    "- **METEOR Score**\r\n",
    "- **Cosine Similarity**\r\n",
    "\r\n",
    "### Steps:\r\n",
    "\r\n",
    "1. **Data Preparation**: The `eval_df` DataFrame is reshaped into a long format with metrics (BERTScore, METEOR, Cosine Similarity) as separate columns.\r\n",
    "2. **Visualization**: \r\n",
    "   - A grouped bar chart is created using `seaborn`, with models on the x-axis and scores on the y-axis.\r\n",
    "   - Bars are color-coded by metric, and annotations display exact values for each bar.\r\n",
    "   - X-axis labels are rotated for readability, and a legend distinguishes metrics.\r\n",
    "\r\n",
    "### Outcome:\r\n",
    "The chart provides a clear visual comparison of model performance across various evaluation metrics, helping assess the strengths of each model.\r\n",
    "heir answer generation quality.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15512571-688f-4da8-8041-552b8591aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Reshaping the data into a long format\n",
    "melted_scores = eval_df.melt(id_vars=\"Model\", \n",
    "                                    value_vars=[ 'BERTScore_Precision', 'BERTScore_Recall', 'BERTScore_F1', 'METEOR', 'Cosine_Similarity'],\n",
    "                                    var_name=\"Metric\", \n",
    "                                    value_name=\"Score\")\n",
    "\n",
    "# Set the plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the grouped bar chart\n",
    "plt.figure(figsize=(24, 8))\n",
    "ax = sns.barplot(x=\"Model\", y=\"Score\", hue=\"Metric\", data=melted_scores, palette=\"Set2\")\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Comparison of BERT, METEOR &  Cosine Similarity Scores for Different Models\", fontsize=16)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add annotations for each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=12, color='black',\n",
    "                xytext=(0, 8), textcoords='offset points')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend(title=\"Metric\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
